{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from importlib import reload\n",
    "import IPython\n",
    "mpl.rcParams['lines.linewidth'] = 0.25\n",
    "mpl.rcParams['axes.spines.top'] = False\n",
    "mpl.rcParams['axes.spines.right'] = False\n",
    "mpl.rcParams['axes.linewidth'] = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, argparse, os, shutil, inspect, json, numpy, math\n",
    "import netdissect\n",
    "from netdissect.easydict import EasyDict\n",
    "from experiment import intervention_experiment\n",
    "from experiment import dissect_experiment\n",
    "from experiment.intervention_experiment import sharedfile\n",
    "from experiment.dissect_experiment import make_upfn\n",
    "from experiment import setting\n",
    "from netdissect import pbar, nethook, renormalize, parallelfolder, pidfile\n",
    "from netdissect import upsample, tally, imgviz, imgsave, bargraph, show\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "# choices are alexnet, vgg16, or resnet152.\n",
    "args = EasyDict(model='vgg16', dataset='places', seg='netpqc', quantile=0.01, layer='conv5_3')\n",
    "resdir = 'results/%s-%s-%s-%s-%d' % (args.model, args.dataset, args.seg, args.layer, args.quantile*1000)\n",
    "def resfile(f):\n",
    "    return os.path.join(resdir, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nethook.InstrumentedModel(setting.load_classifier(args.model).cuda())\n",
    "layername = 'features.conv5_3'\n",
    "model.retain_layer(layername)\n",
    "dataset = setting.load_dataset('places')\n",
    "upfn = make_upfn(args, dataset, model, layername)\n",
    "sample_size = len(dataset)\n",
    "percent_level = 1 - args.quantile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifier labels\n",
    "from urllib.request import urlopen\n",
    "from netdissect import renormalize\n",
    "\n",
    "# synset_url = 'http://gandissect.csail.mit.edu/models/categories_places365.txt'\n",
    "# classlabels = [r.split(' ')[0][3:] for r in urlopen(synset_url).read().decode('utf-8').split('\\n')]\n",
    "classlabels = dataset.classes\n",
    "segmodel, seglabels, segcatlabels = setting.load_segmenter('netpqc')\n",
    "renorm = renormalize.renormalizer(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the segmodel for segmentations.  With broden, we could use ground truth instead.\n",
    "def compute_conditional_indicator(batch, *args):\n",
    "    image_batch = batch.cuda()\n",
    "    seg = segmodel.segment_batch(renorm(image_batch), downsample=4)\n",
    "    _ = model(image_batch)\n",
    "    acts = model.retained_layer(layername)\n",
    "    hacts = upfn(acts)\n",
    "    iacts = (hacts > level_at_99).float() # indicator\n",
    "    return tally.conditional_samples(iacts, seg)\n",
    "pbar.descnext('condi99')\n",
    "condi99 = tally.tally_conditional_mean(compute_conditional_indicator,\n",
    "        dataset, sample_size=sample_size,\n",
    "        num_workers=3, pin_memory=True,\n",
    "        cachefile=resfile('condi99.npz'))\n",
    "iou_99 = tally.iou_from_conditional_indicator_mean(condi99)\n",
    "unit_label_99 = [\n",
    "        (concept.item(), seglabels[concept], segcatlabels[concept], bestiou.item())\n",
    "        for (bestiou, concept) in zip(*iou_99.max(0))]\n",
    "label_list = [label for concept, label, labelcat, iou in unit_label_99 if iou > 0.025]\n",
    "labelcat_list = [labelcat for concept, label, labelcat, iou in unit_label_99 if iou > 0.025]\n",
    "display(IPython.display.SVG(dissect_experiment.graph_conceptcatlist(labelcat_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_precision, baseline_recall, baseline_accuracy, baseline_ba  = (\n",
    "    intervention_experiment.test_perclass_pra(\n",
    "        model, dataset,\n",
    "        cachefile=sharedfile('pra-%s-%s/pra_baseline.npz'\n",
    "            % (args.model, args.dataset))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_recall.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seat_units = [u for u, [_, label, _, _] in enumerate(unit_label_99) if label.startswith('seat')]\n",
    "seat_units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all single-unit ablation accuracy\n",
    "num_units = dict(vgg16=512, alexnet=256)[args.model]\n",
    "single_unit_ablation_acc = torch.zeros(num_units, len(classlabels))\n",
    "single_unit_ablation_ba = torch.zeros(num_units, len(classlabels))\n",
    "single_unit_ablation_precision = torch.zeros(num_units, len(classlabels))\n",
    "single_unit_ablation_recall = torch.zeros(num_units, len(classlabels))\n",
    "\n",
    "for unit in range(num_units):\n",
    "    [single_unit_ablation_precision[unit], single_unit_ablation_recall[unit], single_unit_ablation_acc[unit], single_unit_ablation_ba[unit]\n",
    "    ] = intervention_experiment.test_perclass_pra(\n",
    "                model, dataset,\n",
    "                layername=layername,\n",
    "                ablated_units=[unit],\n",
    "                cachefile=sharedfile('pra-%s-%s/pra_ablate_unit_%d.npz' %\n",
    "                    (args.model, args.dataset, unit)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_unit_ablation_acc[196,70]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for classnum in range(len(classlabels)):\n",
    "    for unit in single_unit_ablation_ba[:,classnum].sort(0)[1]:\n",
    "        diff = single_unit_ablation_ba[unit, classnum] - baseline_ba[classnum]\n",
    "        if diff > -0.01:\n",
    "            break\n",
    "        print('%s: unit %d (%s) -> %.3f' % (\n",
    "            classlabels[classnum], unit, unit_label_99[unit][1],\n",
    "            diff ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and reload\n",
    "numpy.savez(resfile('unit_ablation.npz'),\n",
    "            single_unit_ablation_ba=single_unit_ablation_ba,\n",
    "            baseline_ba=baseline_ba)\n",
    "print(os.path.abspath(resfile('unit_ablation.npz')))\n",
    "data = numpy.load(resfile('unit_ablation.npz'))\n",
    "sua = torch.from_numpy(data['single_unit_ablation_ba'])\n",
    "base = torch.from_numpy(data['baseline_ba'])\n",
    "for classnum in range(len(classlabels)):\n",
    "    for unit in sua[:,classnum].sort(0)[1]:\n",
    "        diff = sua[unit, classnum] - base[classnum]\n",
    "        if diff > -0.01:\n",
    "            break\n",
    "        print('%s: unit %d (%s) -> %.3f' % (\n",
    "            classlabels[classnum], unit, unit_label_99[unit][1],\n",
    "            diff ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = setting.load_dataset('places', 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttv_baseline_precision, ttv_baseline_recall, ttv_baseline_accuracy, ttv_baseline_ba  = (\n",
    "    intervention_experiment.test_perclass_pra(\n",
    "        model, train_dataset,\n",
    "        sample_size=sample_size,\n",
    "        cachefile=sharedfile('ttv-pra-%s-%s/pra_train_baseline.npz'\n",
    "            % (args.model, args.dataset))))\n",
    "    \n",
    "ttv_single_unit_ablation_ba = torch.zeros(num_units, len(classlabels))\n",
    "for unit in range(512):\n",
    "    pbar.descnext('test unit %d' % unit)\n",
    "    _, _, _, ablation_ba = intervention_experiment.test_perclass_pra(\n",
    "            model, train_dataset,\n",
    "            layername=layername,\n",
    "            ablated_units=[unit],\n",
    "            sample_size=sample_size,\n",
    "            cachefile=\n",
    "                sharedfile('ttv-pra-%s-%s/pra_train_ablate_unit_%d.npz' %\n",
    "                (args.model, args.dataset, unit)))\n",
    "    minacc, minclass = (ablation_ba - baseline_ba).min(0)\n",
    "    ttv_single_unit_ablation_ba[unit] = ablation_ba\n",
    "        \n",
    "ttv_ablate_salient = [0.0]\n",
    "ttv_ablate_nonsalient = [0.0]\n",
    "\n",
    "classnum = classlabels.index('ski_resort')\n",
    "for num_salient in range(1, 512):\n",
    "    unitlist = ttv_single_unit_ablation_ba[:,classnum].sort(0)[1][:num_salient]\n",
    "    _, _, _, testba = intervention_experiment.test_perclass_pra(model, dataset,\n",
    "            layername=layername,\n",
    "            ablated_units=unitlist,\n",
    "            cachefile=sharedfile('ttv-pra-%s-%s/pra_val_ablate_classunits_%s_ba_%d.npz' %\n",
    "                                 (args.model, args.dataset, classlabels[classnum], len(unitlist))))\n",
    "    # print([(classlabels[c], d.item()) for d, c in list(zip(*(testba - baseline_ba).sort(0)))[:5]])\n",
    "    ttv_ablate_salient.append((testba[classnum] - baseline_ba[classnum]).item())\n",
    "\n",
    "    unitlist = ttv_single_unit_ablation_ba[:,classnum].sort(0)[1][-num_salient:]\n",
    "    _, _, _, testba2 = intervention_experiment.test_perclass_pra(model, dataset,\n",
    "            layername=layername,\n",
    "            ablated_units=unitlist,\n",
    "            cachefile=sharedfile('ttv-pra-%s-%s/pra_val_ablate_classunits_%s_worstba_%d.npz' %\n",
    "                                 (args.model, args.dataset, classlabels[classnum], len(unitlist))))\n",
    "    # print([(classlabels[c], d.item()) for d, c in list(zip(*(testba2 - baseline_ba).sort(0)))[:5]])\n",
    "    ttv_ablate_nonsalient.append((testba2[classnum] - baseline_ba[classnum]).item())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.ticker as mtick\n",
    "\n",
    "plt.style.use('dark_background')\n",
    "\n",
    "classnum = classlabels.index('ski_resort')\n",
    "b = baseline_ba[classnum].item()\n",
    "\n",
    "#fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(6, 1.7), dpi=300)\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(5.9, 3.1), dpi=300)\n",
    "#ax.axvline(20, color='gray', linewidth=0.5, linestyle='-')\n",
    "#ax.axvline(492, color='gray', linewidth=0.5, linestyle='-')\n",
    "ax.axhline(b, color='gray', linewidth=0.5, linestyle='-')\n",
    "\n",
    "\n",
    "ax.plot([y+b for y in ttv_ablate_salient], linewidth=1, label='Removing the most important units together',\n",
    "       c=\"#4B4CBF\")\n",
    "ax.plot([y+b for y in ttv_ablate_nonsalient] + [0.5], linewidth=1, label='Removing all but the most important units',\n",
    "       c=\"#F0883B\")\n",
    "if True:\n",
    "    ax.scatter([0, 20, 492], [b, b+ttv_ablate_salient[20], b+ttv_ablate_nonsalient[492]],\n",
    "           color=['#55B05B', \"#4B4CBF\", \"#F0883B\"], zorder=10,s=50)\n",
    "#    ax.scatter([0, 20, ], [b, b+ttv_ablate_salient[20], ],\n",
    "#           color=['#55B05B', \"#4B4CBF\", ], zorder=10,s=50)\n",
    "else:\n",
    "    ax.scatter([0, 2, 510], [b, b+ttv_ablate_salient[2], b+ttv_ablate_nonsalient[510]],\n",
    "           color=['#55B05B', \"#4B4CBF\", \"#F0883B\"], zorder=10,s=50)\n",
    "ax.set_xticks([0, 20, 128, 256, 384, 492, 512])\n",
    "ax.set_xticklabels([0, 20, 128, 256, 384, '492   ', '    512'])\n",
    "# ax.set_yticklabels(['40%', '50%', '60%', '70%', '80%', '90%'])\n",
    "#ax.set_yticks([0.5, 0.65, 0.8])\n",
    "ax.set_yticks([0.5, 0.6, 0.7, 0.8, 0.9, 1.0])\n",
    "ax.yaxis.set_major_formatter(mtick.PercentFormatter(xmax=1.0))\n",
    "ax.set_xlabel('number of conv5_3 units removed together')\n",
    "ax.set_ylabel('single-class accuracy')\n",
    "ax.set_ylim(0.5, 1.0)\n",
    "ax.legend(loc='center right', bbox_to_anchor=(0.95, 0.3))\n",
    "plt.savefig(\"ice-one-class.pdf\", bbox_inches='tight')\n",
    "\n",
    "print(b, b+ttv_ablate_salient[20], b+ttv_ablate_nonsalient[492])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_dataset), len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b + ttv_ablate_salient[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttv_best_ba = []\n",
    "ttv_worst_ba = []\n",
    "ttv_base_ba = []\n",
    "ttv_best_ablate = []\n",
    "ttv_worst_ablate = []\n",
    "\n",
    "# Change this to 256 to see supplemental results\n",
    "num_best_units = 20\n",
    "\n",
    "for classnum in range(len(classlabels)):\n",
    "    unitlist = ttv_single_unit_ablation_ba[:,classnum].sort(0)[1][:num_best_units]\n",
    "    _, _, _, testba = intervention_experiment.test_perclass_pra(model, dataset,\n",
    "            layername=layername,\n",
    "            ablated_units=unitlist,\n",
    "            cachefile=sharedfile('ttv-pra-%s-%s/pra_val_ablate_classunits_%s_ba_%d.npz' %\n",
    "                                 (args.model, args.dataset, classlabels[classnum], len(unitlist))))\n",
    "    # print([(classlabels[c], d.item()) for d, c in list(zip(*(testba - baseline_ba).sort(0)))[:5]])\n",
    "    ttv_best_ba.append(testba[classnum].item())\n",
    "    ttv_base_ba.append(baseline_ba[classnum].item())\n",
    "    ttv_best_ablate.append((testba[classnum] - baseline_ba[classnum]).item())\n",
    "    unitlist = ttv_single_unit_ablation_ba[:,classnum].sort(0)[1][num_best_units - 512:]\n",
    "    _, _, _, testba2 = intervention_experiment.test_perclass_pra(model, dataset,\n",
    "            layername=layername,\n",
    "            ablated_units=unitlist,\n",
    "            cachefile=sharedfile('ttv-pra-%s-%s/pra_val_ablate_classunits_%s_worstba_%d.npz' %\n",
    "                                 (args.model, args.dataset, classlabels[classnum], len(unitlist))))\n",
    "    # print([(classlabels[c], d.item()) for d, c in list(zip(*(testba2 - baseline_ba).sort(0)))[:5]])\n",
    "    ttv_worst_ba.append(testba2[classnum].item())\n",
    "    ttv_worst_ablate.append((testba2[classnum] - baseline_ba[classnum]).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import random\n",
    "\n",
    "fig, [ax1, ax2] =plt.subplots(nrows=2, ncols=1, figsize=(6.5, 3), dpi=300,\n",
    "                              sharex='all')\n",
    "\n",
    "ax2.scatter(ttv_worst_ba, range(len(ttv_worst_ba)), alpha=0.5, s=10, c='#F0883B')\n",
    "ax2.scatter(ttv_best_ba, range(len(ttv_best_ba)), alpha=0.5, s=10, c='#4B4CBF')\n",
    "ax2.scatter(ttv_base_ba, range(len(ttv_base_ba)), alpha=0.5, s=10, c='#55B05B')\n",
    "ax2.get_yaxis().set_ticks([])\n",
    "ax2.set_ylabel('Scene class')\n",
    "\n",
    "\n",
    "ax1.axvline(numpy.array(ttv_best_ba).mean().item(), color='#B6B6F2', linewidth=1.5, linestyle='--')\n",
    "ax1.axvline(numpy.array(ttv_worst_ba).mean().item(), color='#F2CFB6', linewidth=1.5, linestyle='--')\n",
    "ax1.axvline(numpy.array(ttv_base_ba).mean().item(), color='#B6F2BA', linewidth=1.5, linestyle='--')\n",
    "\n",
    "\n",
    "sns.distplot(ttv_base_ba, kde=True, hist=False, kde_kws = {'linewidth': 3, \"color\":'#55B05B'},\n",
    "             label=\"No units removed, mean class accuracy=%.1f%%\" % (100*numpy.array(ttv_base_ba).mean().item()),\n",
    "            ax=ax1)\n",
    "if True:\n",
    "    sns.distplot(ttv_best_ba, kde=True, hist=False, kde_kws = {'linewidth': 3, \"color\":\"#4B4CBF\"},\n",
    "             label=\"%d units most damaging to class, mean=%.1f%%\" %\n",
    "             (num_best_units, 100*numpy.array(ttv_best_ba).mean().item()),\n",
    "            ax=ax1)\n",
    "sns.distplot(ttv_worst_ba, kde=True, hist=False, kde_kws = {'linewidth': 3, \"color\":\"#F0883B\"},\n",
    "             label=\"All %d other units removed, mean=%.1f%%\" %\n",
    "             (512 - num_best_units, 100*numpy.array(ttv_worst_ba).mean().item()),\n",
    "            ax=ax1)\n",
    "ax1.set_ylabel('Density')\n",
    "ax1.get_yaxis().set_ticks([])\n",
    "ax1.set_ylim([0, 20])\n",
    "ax2.set_xlabel('Balanced single-class accuracy when sets of units are removed')\n",
    "ax1.set_xlim(0.48, 1.02)\n",
    "ax2.set_xticklabels(['40%', '50%', '60%', '70%', '80%', '90%', '100%'])\n",
    "legend = ax1.legend(loc='upper right', bbox_to_anchor=(1, 1.1))\n",
    "# legend.get_frame().set_facecolor('none')\n",
    "legend.get_frame().set_edgecolor('none')\n",
    "plt.savefig(\"ice-all-classes.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import random\n",
    "\n",
    "fig, [ax1, ax2] =plt.subplots(nrows=2, ncols=1, figsize=(8, 3), dpi=300,\n",
    "                              sharex='all')\n",
    "\n",
    "ax2.scatter(ttv_worst_ablate, range(len(ttv_worst_ablate)), alpha=0.5, s=10, c='#F0883B')\n",
    "ax2.scatter(ttv_best_ablate, range(len(ttv_best_ablate)), alpha=0.5, s=10, c='#4B4CBF')\n",
    "ax2.scatter([0] * len(ttv_best_ablate), range(len(ttv_best_ablate)), alpha=0.05, s=10, c='#55B05B')\n",
    "ax2.get_yaxis().set_ticks([])\n",
    "ax2.set_ylabel('Scene class')\n",
    "\n",
    "\n",
    "ax1.axvline(numpy.array(ttv_best_ablate).mean().item(), color='#B6B6F2', linewidth=1.5, linestyle='--')\n",
    "ax1.axvline(numpy.array(ttv_worst_ablate).mean().item(), color='#F2CFB6', linewidth=1.5, linestyle='--')\n",
    "\n",
    "\n",
    "sns.distplot(ttv_best_ablate, kde=True, hist=False, kde_kws = {'linewidth': 3, \"color\":\"#4B4CBF\"},\n",
    "             label=\"Removed most-important %d units, mean=%.1f%%\" %\n",
    "             (num_best_units, 100*numpy.array(ttv_best_ba).mean().item()),\n",
    "             ax=ax1)\n",
    "sns.distplot(ttv_worst_ablate, kde=True, hist=False, kde_kws = {'linewidth': 3, \"color\":\"#F0883B\"},\n",
    "             label=\"Kept only most-important %d units, mean=%.1f%%\" %\n",
    "             (num_best_units, 100*numpy.array(ttv_worst_ba).mean().item()),\n",
    "            ax=ax1)\n",
    "ax1.axvline(0, color='#55B05B', linewidth=3, linestyle='-', label='No units removed')\n",
    "ax1.set_ylabel('Density')\n",
    "ax1.get_yaxis().set_ticks([])\n",
    "ax1.set_ylim([0, 7])\n",
    "ax2.set_xlabel('Change in balanced single-class accuracy change when sets of units are removed')\n",
    "ax1.set_xlim(-0.57, 0.3)\n",
    "ax2.set_xticklabels(['-0.6', '-0.5', '-0.4', '-0.3', '-0.2', '-0.1', 'no change', 0.1, 0.2, 0.3])\n",
    "#legend = ax1.legend()\n",
    "legend = ax1.legend(loc='upper left', bbox_to_anchor=(-0.01, 1.25))\n",
    "\n",
    "legend.get_frame().set_facecolor('none')\n",
    "legend.get_frame().set_edgecolor('none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy.savez(resfile('ttv_unit_ablation.npz'),\n",
    "        single_unit_ablation_ba=ttv_single_unit_ablation_ba,\n",
    "        baseline_ba=ttv_baseline_ba)\n",
    "\n",
    "results = {}\n",
    "for classnum in range(len(classlabels)):\n",
    "    unitlist = []\n",
    "    for unit in ttv_single_unit_ablation_ba[:,classnum].sort(0)[1]:\n",
    "        diff = ttv_single_unit_ablation_ba[unit, classnum] - ttv_baseline_ba[classnum]\n",
    "        if diff > -0.005:\n",
    "            break\n",
    "        print('%s: unit %d -> %.3f' % (\n",
    "            classlabels[classnum], unit, diff ))\n",
    "        unitlist.append({'unit': unit.item(), 'val_acc': diff.item()})\n",
    "    results[classlabels[classnum]] = unitlist\n",
    "with open(resfile('ttv_unit_ablation.json'), 'w') as f:\n",
    "    json.dump(results, f, indent=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_topN_accuracy(img, cls):\n",
    "    pred = model(img.cuda())\n",
    "    scores, choices = pred.sort(1)\n",
    "    correct = (choices.flip(1) == cls.cuda()[:,None].expand(choices.shape)).float()\n",
    "    cum_correct = correct.cumsum(1)\n",
    "    return cum_correct\n",
    "\n",
    "topN_acc = tally.tally_mean(calculate_topN_accuracy, dataset, batch_size=100, pin_memory=True,\n",
    "                    cachefile=sharedfile('pra-%s-%s/topn_accuracy.npz'\n",
    "                    % (args.model, args.dataset)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topN_acc.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "for i, (im, c) in enumerate(dataset):\n",
    "    if i >= 10:\n",
    "        break\n",
    "    results.append([[i], [renormalize.as_image(im)]])\n",
    "show(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for num_salient in [4, 20, 492]:\n",
    "    unitlist = ttv_single_unit_ablation_ba[:,classnum].sort(0)[1][-num_salient:]\n",
    "    test_pre, test_rec, test_acc, testba2 = intervention_experiment.test_perclass_pra(model, dataset,\n",
    "            layername=layername,\n",
    "            ablated_units=unitlist,\n",
    "            cachefile=sharedfile('ttv-pra-%s-%s/pra_val_ablate_classunits_%s_worstba_%d.npz' %\n",
    "                                 (args.model, args.dataset, classlabels[classnum], len(unitlist))))\n",
    "    print(num_salient, test_rec.mean())\n",
    "print(1/365.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([classlabels[i] for i in test_rec.sort(0)[1].flip(0)[:8]])\n",
    "print(test_rec.sort(0)[0].flip(0)[:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iou_99.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttv_single_unit_ablation_ba.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttv_baseline_ba.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_units = torch.unique((ttv_single_unit_ablation_ba - ttv_baseline_ba[None,:]).min(0)[1])\n",
    "len(important_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iou_99.max(0)[0][important_units].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iou_99.max(0)[0].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_cutoff = 4\n",
    "unit_importance = torch.bincount((ttv_single_unit_ablation_ba - ttv_baseline_ba[None,:]).sort(0)[1][:important_cutoff].view(-1))\n",
    "most_important_units = (unit_importance >= 7).nonzero()[:,0]\n",
    "print(len(most_important_units))\n",
    "print(most_important_units)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_vals = torch.unique(unit_importance)\n",
    "fig, ax = plt.subplots(figsize=(5,2.5), dpi=300)\n",
    "tail = 7\n",
    "\n",
    "xlist = [i for i in imp_vals.numpy() if i < tail] + [tail]\n",
    "ylist = (\n",
    "    [iou_99.max(0)[0][unit_importance == i].mean().item()\n",
    "         for i in imp_vals if i < tail] +\n",
    "    [iou_99.max(0)[0][unit_importance >= tail].mean().item()])\n",
    "yerr = (\n",
    "    [iou_99.max(0)[0][unit_importance == i].std().item()\n",
    "          / math.sqrt(len(iou_99.max(0)[0][unit_importance == i]))\n",
    "         for i in imp_vals if i < tail] +\n",
    "    [iou_99.max(0)[0][unit_importance >= tail].std().item()\n",
    "        / math.sqrt(len(iou_99.max(0)[0][unit_importance >= tail]))]\n",
    ")\n",
    "ax.bar(xlist, ylist, yerr=yerr, color=\"#4B4CBF\",\n",
    "    error_kw=dict(lw=1, capsize=5, capthick=1)\n",
    ")\n",
    "for x, y in zip(xlist, ylist):\n",
    "    n = (sum(unit_importance==x) if x < tail\n",
    "         else sum(unit_importance>=tail)).item()\n",
    "    plt.text(x=x, y=1e-3, s='n=%d' % n, size=7.6, ha='center', va='baseline', color='white')\n",
    "ax.set_xlabel('number of classes for which unit is important')\n",
    "ax.set_ylabel('mean IoU$_{u,c}$')\n",
    "ax.set_xticks(xlist)\n",
    "ax.set_xticklabels([x if x < tail else '$\\\\geq %d$' % tail for x in xlist ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "imp_vals = torch.unique(unit_importance)\n",
    "fig, ax = plt.subplots(figsize=(6,1.7), dpi=300)\n",
    "tail = 7\n",
    "\n",
    "xlist = [i for i in imp_vals.numpy() if i < tail] + [tail]\n",
    "ylist = (\n",
    "    [iou_99.max(0)[0][unit_importance == i].mean().item()\n",
    "         for i in imp_vals if i < tail] +\n",
    "    [iou_99.max(0)[0][unit_importance >= tail].mean().item()])\n",
    "yerr = (\n",
    "    [iou_99.max(0)[0][unit_importance == i].std().item()\n",
    "          / math.sqrt(len(iou_99.max(0)[0][unit_importance == i]))\n",
    "         for i in imp_vals if i < tail] +\n",
    "    [iou_99.max(0)[0][unit_importance >= tail].std().item()\n",
    "        / math.sqrt(len(iou_99.max(0)[0][unit_importance >= tail]))]\n",
    ")\n",
    "ax.barh(xlist,ylist, xerr=yerr, color=\"#4B4CBF\",\n",
    "    error_kw=dict(lw=1, capsize=2, capthick=1)\n",
    ")\n",
    "if True:\n",
    "    for x, y in zip(xlist, ylist):\n",
    "        n = (sum(unit_importance==x) if x < tail\n",
    "             else sum(unit_importance>=tail)).item()\n",
    "        plt.text(y=x, x=1e-3, s='n=%d' % n, size=7.6, ha='left', va='center', color='white')\n",
    "ax.set_ylabel('classes for which\\nunit is top-%d imp' % important_cutoff)\n",
    "ax.set_ylabel('classes for which\\nunit is important')\n",
    "ax.set_xlabel('mean IoU$_{u,c}$ of units')\n",
    "ax.set_yticks(xlist)\n",
    "ax.set_yticklabels([x if x < tail else '$\\geq %d$' % tail for x in xlist ])\n",
    "plt.savefig(\"ice-vs-iou.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((ttv_single_unit_ablation_ba - ttv_baseline_ba[None,:]).sort(0)[1] == 150).nonzero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ydata = [\n",
    "    unit_label_99[u][-1]\n",
    "    for u in (ttv_single_unit_ablation_ba - ttv_baseline_ba[None,:]).mean(1).sort(0)[1]\n",
    "]\n",
    "xdata = (ttv_single_unit_ablation_ba - ttv_baseline_ba[None,:]).mean(1).sort(0)[0].numpy()\n",
    "\n",
    "bsize = 32\n",
    "ybatch = [numpy.mean(ydata[i:i+bsize]) for i in range(0, 512, bsize)]\n",
    "yerr = [numpy.std(ydata[i:i+bsize] / numpy.sqrt(bsize)) for i in range(0, 512, bsize)]\n",
    "xbatch = [-numpy.mean(xdata[i:i+bsize]) for i in range(0, 512, bsize)]\n",
    "xerr = [numpy.std(xdata[i:i+bsize] / numpy.sqrt(bsize)) for i in range(0, 512, bsize)]\n",
    "fig, ax = plt.subplots(figsize=(5.8, 5.5), dpi=300)\n",
    "ax.plot(xbatch, ybatch, marker='o', color=\"#4B4CBF\", lw=2)\n",
    "ax.errorbar(xbatch, ybatch, yerr=yerr, xerr=xerr, capsize=2, capthick=1, color='black')\n",
    "ax.set_ylabel('mean iou')\n",
    "ax.set_xlabel('mean class importance\\n(class importance of unit averaged over all classes)')\n",
    "legend = ax.legend(['32 units grouped by mean class importance', 'error bars show standard error'], loc='upper left')\n",
    "legend.get_frame().set_facecolor('none')\n",
    "legend.get_frame().set_edgecolor('none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "unit_importance = defaultdict(list)\n",
    "for cls, units in enumerate(\n",
    "    (ttv_single_unit_ablation_ba - ttv_baseline_ba[None,:]).sort(0)[1][:important_cutoff].permute((1, 0))):\n",
    "    for u in units:\n",
    "        unit_importance[u.item()].append(classlabels[cls])\n",
    "unit_importance_records = []\n",
    "for u in range(512):\n",
    "    unit_importance_records.append(dict(unit=u, important_to=unit_importance[u]))\n",
    "with open(resfile('importance.json'), 'w') as f:\n",
    "    json.dump(dict(importance=unit_importance_records), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "G = nx.Graph()\n",
    "added = set()\n",
    "for u in range(512):\n",
    "    if len(unit_importance_records[u]['important_to']) > 1:\n",
    "        G.add_node('%d' % u)\n",
    "        for c in unit_importance_records[u]['important_to']:\n",
    "            if c not in added:\n",
    "                G.add_node(c)\n",
    "            G.add_edge('%d' % u, c)\n",
    "fig, ax = plt.subplots(1, 1, figsize=(50, 50));\n",
    "nx.draw_networkx(G, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(nx.minimum_cycle_basis(G))\n",
    "print(nx.find_cycle(G, \"soccer_field\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find cycles of length 3\n",
    "adjacent_cls = defaultdict(set)\n",
    "for r in unit_importance_records:\n",
    "    for c in r['important_to']:\n",
    "        adjacent_cls[c].update(r['important_to'])\n",
    "set_of_sets = [set(r['important_to']) for r in unit_importance_records]\n",
    "for u in iou_99.max(0)[0].sort(0)[1].flip(0):\n",
    "    r = unit_importance_records[u]\n",
    "    print()\n",
    "    print('unit %d:' % r['unit'])\n",
    "    here = r['important_to']\n",
    "    for i, c in enumerate(here):\n",
    "        for j in range(i + 1, len(here)):\n",
    "            d = here[j]\n",
    "            candidates = adjacent_cls[c].intersection(adjacent_cls[d]).difference(here)\n",
    "            for e in list(candidates):\n",
    "                triple = [c, d, e]\n",
    "                if any(s.issuperset(triple) for s in set_of_sets):\n",
    "                    candidates.remove(e)\n",
    "            if len(candidates):\n",
    "                print(c, d, candidates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure covariance between (max) units and classes\n",
    "\n",
    "def compute_maxact_and_pred(batch, *args):\n",
    "    image_batch = batch.cuda()\n",
    "    preds = torch.nn.functional.softmax(model(image_batch), dim=1)\n",
    "    acts = model.retained_layer(layername)\n",
    "    maxacts = acts.view(acts.shape[0], acts.shape[1], -1).max(2)[0]\n",
    "    return maxacts, preds\n",
    "actpredcov = tally.tally_cross_covariance(compute_maxact_and_pred,\n",
    "        dataset, sample_size=sample_size,\n",
    "        num_workers=3, pin_memory=True,\n",
    "        cachefile=resfile('actpredcov.npz'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actpredcov.correlation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apc = actpredcov.correlation()\n",
    "apc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# important unit per class\n",
    "iupc = (ttv_single_unit_ablation_ba - ttv_baseline_ba[None,:]).sort(0)[1][:important_cutoff].permute((1, 0))\n",
    "niupc = (ttv_single_unit_ablation_ba - ttv_baseline_ba[None,:]).sort(0)[1][important_cutoff:].permute((1, 0))\n",
    "negclass = apc[iupc, torch.arange(365)[:,None]].min(1)[0].min(0)[1]\n",
    "apc[:,negclass][iupc[negclass]], iupc[negclass]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apc[iupc, torch.arange(365)[:,None]].contiguous().view(-1).sort(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "imp = -(ttv_single_unit_ablation_ba - ttv_baseline_ba[None,:])\n",
    "fig, ax = plt.subplots(figsize=(6, 6), dpi=300)\n",
    "#ax.scatter(imp.view(-1), apc.view(-1))\n",
    "ax.scatter(\n",
    "    imp[niupc, torch.arange(365)[:,None]].contiguous().view(-1),\n",
    "    apc[niupc, torch.arange(365)[:,None]].contiguous().view(-1),\n",
    "    s=0.1, alpha=0.2, color=\"#F0883B\")\n",
    "ax.scatter(\n",
    "    imp[iupc, torch.arange(365)[:,None]].contiguous().view(-1),\n",
    "    apc[iupc, torch.arange(365)[:,None]].contiguous().view(-1),\n",
    "    s=1, alpha=0.5, color=\"#4B4CBF\")\n",
    "ax.set_ylabel('correlation between unit and class')\n",
    "ax.set_xlabel('importance of unit to class accuracy')\n",
    "ax.add_patch(\n",
    "     mpl.patches.Rectangle(\n",
    "        (-0.025, 0),\n",
    "        0.05,\n",
    "        -0.15,\n",
    "        linewidth=1,edgecolor='r',facecolor='none'\n",
    "     ) )\n",
    "ax.text(0.07, 0.1, '%.1f%% of important-unit-class\\ncorrelations are positive'\n",
    "        % (100*(apc[iupc, torch.arange(365)[:,None]] > 0).sum().double() / iupc.numel()))\n",
    "ax.text(0.03, -0.1, '%.1f%% of all unit-class\\ncorrelations are negative'\n",
    "        % (100*((apc < 0).sum().float() / apc.numel())))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(apc < 0).sum().float() / apc.numel()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}