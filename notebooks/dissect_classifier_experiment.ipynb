{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Dissection (for classifiers)\n",
    "\n",
    "In this notebook, we will examine internal layer representations for a classifier trained to recognize scene categories.\n",
    "\n",
    "Setup matplotlib, torch, and numpy for a high-resolution browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from importlib import reload\n",
    "import IPython\n",
    "mpl.rcParams['lines.linewidth'] = 0.25\n",
    "mpl.rcParams['axes.spines.top'] = False\n",
    "mpl.rcParams['axes.spines.right'] = False\n",
    "mpl.rcParams['axes.linewidth'] = 0.25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up experiment directory and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, argparse, os, shutil, inspect, json, numpy, math\n",
    "import netdissect\n",
    "from netdissect.easydict import EasyDict\n",
    "from netdissect import pbar, nethook, renormalize, parallelfolder, pidfile\n",
    "from netdissect import upsample, tally, imgviz, imgsave, bargraph, show\n",
    "from experiment import dissect_experiment as experiment\n",
    "\n",
    "# choices are alexnet, vgg16, or resnet152.\n",
    "args = EasyDict(model='vgg16', dataset='places', seg='netpqc', layer='conv5_3', quantile=0.01)\n",
    "resdir = 'results/%s-%s-%s-%s-%s' % (args.model, args.dataset, args.seg, args.layer, int(args.quantile * 1000))\n",
    "def resfile(f):\n",
    "    return os.path.join(resdir, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load classifier model and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = experiment.load_model(args)\n",
    "layername = experiment.instrumented_layername(args)\n",
    "model.retain_layer(layername)\n",
    "dataset = experiment.load_dataset(args)\n",
    "upfn = experiment.make_upfn(args, dataset, model, layername)\n",
    "sample_size = len(dataset)\n",
    "percent_level = 1.0 - args.quantile\n",
    "\n",
    "print('Inspecting layer %s of model %s on %s' % (layername, args.model, args.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load segmenter, segment labels, classifier labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifier labels\n",
    "from urllib.request import urlopen\n",
    "from netdissect import renormalize\n",
    "\n",
    "# synset_url = 'http://gandissect.csail.mit.edu/models/categories_places365.txt'\n",
    "# classlabels = [r.split(' ')[0][3:] for r in urlopen(synset_url).read().decode('utf-8').split('\\n')]\n",
    "classlabels = dataset.classes\n",
    "segmodel, seglabels, segcatlabels = experiment.setting.load_segmenter(args.seg)\n",
    "renorm = renormalize.renormalizer(dataset, target='zc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test classifier on some images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from netdissect import renormalize\n",
    "\n",
    "indices = [200, 755, 709, 423, 60, 100, 110, 120]\n",
    "batch = torch.cat([dataset[i][0][None,...] for i in indices])\n",
    "truth = [classlabels[dataset[i][1]] for i in indices]\n",
    "preds = model(batch.cuda()).max(1)[1]\n",
    "imgs = [renormalize.as_image(t, source=dataset) for t in batch]\n",
    "prednames = [classlabels[p.item()] for p in preds]\n",
    "show([[img, 'pred: ' + pred, 'true: ' + gt] for img, pred, gt in zip(imgs, prednames, truth)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "segment single image, and visualize the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from netdissect import imgviz\n",
    "\n",
    "iv = imgviz.ImageVisualizer(120, source=dataset)\n",
    "seg = segmodel.segment_batch(renorm(batch).cuda(), downsample=4)\n",
    "\n",
    "show([(iv.image(batch[i]), iv.segmentation(seg[i,0]),\n",
    "            iv.segment_key(seg[i,-1], segmodel))\n",
    "            for i in range(len(seg))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "visualize activations for single layer of single image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from netdissect import imgviz\n",
    "\n",
    "acts = model.retained_layer(layername).cpu()\n",
    "ivsmall = imgviz.ImageVisualizer((100, 100), source=dataset)\n",
    "display(show.blocks(\n",
    "    [[[ivsmall.masked_image(batch[0], acts, (0, u), percent_level=0.99)],\n",
    "      [ivsmall.heatmap(acts, (0, u), mode='nearest')]] for u in range(min(acts.shape[1], 12))]\n",
    "))\n",
    "\n",
    "num_units = acts.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect quantile statistics\n",
    "\n",
    "First, unconditional quantiles over the activations.  We will upsample them to 56x56 to match with segmentations later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbar.descnext('rq')\n",
    "def compute_samples(batch, *args):\n",
    "    image_batch = batch.cuda()\n",
    "    _ = model(image_batch)\n",
    "    acts = model.retained_layer(layername)\n",
    "    hacts = upfn(acts)\n",
    "    return hacts.permute(0, 2, 3, 1).contiguous().view(-1, acts.shape[1])\n",
    "rq = tally.tally_quantile(compute_samples, dataset,\n",
    "                          sample_size=sample_size,\n",
    "                          r=8192,\n",
    "                          num_workers=100,\n",
    "                          pin_memory=True,\n",
    "                          cachefile=resfile('rq.npz'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Units\n",
    "\n",
    "Collect topk stats first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbar.descnext('topk')\n",
    "def compute_image_max(batch, *args):\n",
    "    image_batch = batch.cuda()\n",
    "    _ = model(image_batch)\n",
    "    acts = model.retained_layer(layername)\n",
    "    acts = acts.view(acts.shape[0], acts.shape[1], -1)\n",
    "    acts = acts.max(2)[0]\n",
    "    return acts\n",
    "topk = tally.tally_topk(compute_image_max, dataset, sample_size=sample_size,\n",
    "        batch_size=50, num_workers=30, pin_memory=True,\n",
    "        cachefile=resfile('topk.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single image visualization\n",
    "print(topk.result()[1][10][6], dataset.images[topk.result()[1][10][6]])\n",
    "image_number = topk.result()[1][10][4].item()\n",
    "unit_number = 10\n",
    "iv = imgviz.ImageVisualizer((224, 224), source=dataset, quantiles=rq,\n",
    "        level=rq.quantiles(percent_level))\n",
    "batch = torch.cat([dataset[i][0][None,...] for i in [image_number]])\n",
    "truth = [classlabels[dataset[i][1]] for i in [image_number]]\n",
    "preds = model(batch.cuda()).max(1)[1]\n",
    "imgs = [renormalize.as_image(t, source=dataset) for t in batch]\n",
    "prednames = [classlabels[p.item()] for p in preds]\n",
    "acts = model.retained_layer(layername)\n",
    "show([[img, 'pred: ' + pred, 'true: ' + gt] for img, pred, gt in zip(imgs, prednames, truth)])\n",
    "show([[iv.masked_image(batch[0], acts, (0, unit_number))]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we just need to run through and visualize the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbar.descnext('unit_images')\n",
    "\n",
    "iv = imgviz.ImageVisualizer((100, 100), source=dataset, quantiles=rq,\n",
    "        level=rq.quantiles(percent_level))\n",
    "def compute_acts(image_batch):\n",
    "    image_batch = image_batch.cuda()\n",
    "    _ = model(image_batch)\n",
    "    acts_batch = model.retained_layer(layername)\n",
    "    return acts_batch\n",
    "unit_images = iv.masked_images_for_topk(\n",
    "        compute_acts, dataset, topk, k=5, num_workers=30, pin_memory=True,\n",
    "        cachefile=resfile('top5images.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for u in [10, 20, 30, 40, 19, 190]:\n",
    "    print('unit %d' % u)\n",
    "    display(unit_images[u])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Units\n",
    "\n",
    "Collect 99% quantile stats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "level_at_99 = rq.quantiles(percent_level).cuda()[None,:,None,None]\n",
    "# Use the segmodel for segmentations.  With broden, we could use ground truth instead.\n",
    "def compute_conditional_indicator(batch, *args):\n",
    "    image_batch = batch.cuda()\n",
    "    seg = segmodel.segment_batch(renorm(image_batch), downsample=4)\n",
    "    _ = model(image_batch)\n",
    "    acts = model.retained_layer(layername)\n",
    "    hacts = upfn(acts)\n",
    "    iacts = (hacts > level_at_99).float() # indicator\n",
    "    return tally.conditional_samples(iacts, seg)\n",
    "pbar.descnext('condi99')\n",
    "condi99 = tally.tally_conditional_mean(compute_conditional_indicator,\n",
    "        dataset, sample_size=sample_size,\n",
    "        num_workers=3, pin_memory=True,\n",
    "        cachefile=resfile('condi99.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iou_99 = tally.iou_from_conditional_indicator_mean(condi99)\n",
    "unit_label_99 = [\n",
    "        (concept.item(), seglabels[concept], segcatlabels[concept], bestiou.item())\n",
    "        for (bestiou, concept) in zip(*iou_99.max(0))]\n",
    "label_list = [labelcat for concept, label, labelcat, iou in unit_label_99 if iou > 0.04]\n",
    "display(IPython.display.SVG(experiment.graph_conceptcatlist(label_list)))\n",
    "len(label_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show a few units with their labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for u in [10, 20, 30, 40]:\n",
    "    print('unit %d, label %s, iou %.3f' % (u, unit_label_99[u][1], unit_label_99[u][3]))\n",
    "    display(unit_images[u])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}