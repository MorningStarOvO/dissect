{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from importlib import reload\n",
    "import IPython\n",
    "mpl.rcParams['lines.linewidth'] = 0.25\n",
    "mpl.rcParams['axes.spines.top'] = False\n",
    "mpl.rcParams['axes.spines.right'] = False\n",
    "mpl.rcParams['axes.linewidth'] = 0.25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up experiment directory and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, argparse, os, shutil, inspect, json, numpy\n",
    "import netdissect\n",
    "from netdissect.easydict import EasyDict\n",
    "from netdissect import experiment\n",
    "from netdissect.experiment import resfile\n",
    "from netdissect import pbar, nethook, renormalize, parallelfolder, pidfile\n",
    "from netdissect import upsample, tally, imgviz, imgsave, bargraph, show\n",
    "\n",
    "# choices are alexnet, vgg16, or resnet152.\n",
    "args = EasyDict(model='vgg16', dataset='places', seg='netpqc', layer=None)\n",
    "resdir = 'results/%s-%s-%s' % (args.model, args.dataset, args.seg)\n",
    "experiment.set_result_dir(resdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load classifier model and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = experiment.load_model(args)\n",
    "layername = experiment.instrumented_layername(args)\n",
    "model.retain_layer(layername)\n",
    "dataset = experiment.load_dataset(args)\n",
    "upfn = experiment.make_upfn(args, dataset, model, layername)\n",
    "sample_size = len(dataset)\n",
    "\n",
    "print('Inspecting layer %s of model %s on %s' % (layername, args.model, args.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifier labels\n",
    "from urllib.request import urlopen\n",
    "from netdissect import renormalize\n",
    "\n",
    "percent_level=0.995\n",
    "classlabels = dataset.classes\n",
    "renorm = renormalize.renormalizer(dataset, mode='zc')\n",
    "pbar.descnext('rq')\n",
    "def compute_samples(batch, *args):\n",
    "    image_batch = batch.cuda()\n",
    "    _ = model(image_batch)\n",
    "    acts = model.retained_layer(layername)\n",
    "    hacts = upfn(acts)\n",
    "    return hacts.permute(0, 2, 3, 1).contiguous().view(-1, acts.shape[1])\n",
    "rq = tally.tally_quantile(compute_samples, dataset,\n",
    "                          sample_size=sample_size,\n",
    "                          r=8192,\n",
    "                          num_workers=100,\n",
    "                          pin_memory=True,\n",
    "                          cachefile=resfile('rq.npz'))\n",
    "from netdissect import imgviz\n",
    "iv = imgviz.ImageVisualizer((100, 100), source=dataset, quantiles=rq, level=rq.quantiles(percent_level))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.retain_layer(layername)\n",
    "image_index = 0\n",
    "out = model(dataset[image_index][0][None,...].cuda())\n",
    "\n",
    "print('gt', classlabels[dataset[image_index][1]])\n",
    "print('pred', classlabels[out.max(1)[1][0]])\n",
    "display(renormalize.as_image(dataset[image_index][0], source=dataset))\n",
    "model.retained_layer(layername).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_class = 'bedroom'\n",
    "# target_class_id = \n",
    "for class_id, classlabel in enumerate(classlabels): \n",
    "    if classlabel == target_class:\n",
    "        target_class_id = class_id\n",
    "        print(target_class_id, classlabel)\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_images = 1000\n",
    "image_index = 0\n",
    "good_indices = []\n",
    "filtering_source_class = False\n",
    "while (True):\n",
    "    gt_label = classlabels[dataset.images[image_index][1]]\n",
    "    if filtering_source_class and gt_label != 'ski_resort':\n",
    "        image_index += 1\n",
    "        continue\n",
    "    out = model(dataset[image_index][0][None,...].cuda())\n",
    "    pred_label = classlabels[out.max(1)[1][0]]\n",
    "    if gt_label != target_class and pred_label != target_class:\n",
    "        good_indices.append(image_index)\n",
    "    else:\n",
    "        print('image {:d} gt {:s} pred {:s}'.format(image_index, gt_label, pred_label))\n",
    "    \n",
    "    image_index += 1\n",
    "    if len(good_indices) == num_images: \n",
    "        print('get {:d} images from {:d} candidates'.format(num_images, image_index))\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, urllib.request\n",
    "import json, urllib.request\n",
    "unit_names = json.load(urllib.request.urlopen('http://dissect.csail.mit.edu/results/vgg16-places-netpqc-conv5_3-10/report.json'))\n",
    "data = json.load(urllib.request.urlopen('http://dissect.csail.mit.edu/results/vgg16-places-netpqc-conv5_3-10/ttv_unit_ablation.json'))\n",
    "\n",
    "\n",
    "units = data[target_class]\n",
    "\n",
    "unit_ids = []\n",
    "unit_acc = []\n",
    "for unit in units: \n",
    "    unit_id = unit['unit']\n",
    "    acc = unit['val_acc']\n",
    "    unit_ids.append(unit_id)\n",
    "    unit_acc.append(acc)\n",
    "    label = unit_names['units'][unit_id]['label']\n",
    "    print(unit_id, acc, label, acts_mean_average[unit_id])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(unit_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(acts_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for good_index in good_indices[:10]:\n",
    "    result_path = os.path.join(results_dir, 'image_{:d}_target_{:s}.pkl'.format(good_index, target_class))\n",
    "    data = pickle_load(result_path)\n",
    "    image_id = data['image_id']\n",
    "    target_id = data['target_id']\n",
    "    ori_image = data['ori']\n",
    "    adv_image = data['adv']\n",
    "    image_ori = dataset[image_id][0]\n",
    "    out = model(image_ori[None,...].cuda())\n",
    "    acts_ori = model.retained_layer(layername).cpu()\n",
    "\n",
    "    adv = renormalize.as_tensor(adv_image, source='pt', mode='imagenet')[None,...]\n",
    "    pred_adv = model(adv.cuda())\n",
    "\n",
    "    acts_adv = model.retained_layer(layername).cpu()\n",
    "    acts_mean = (acts_adv - acts_ori).mean(dim=(2, 3)).numpy()[0]\n",
    "\n",
    "    diff_image = adv_image - ori_image\n",
    "    diff_image = diff_image/abs(diff_image).max()+0.5\n",
    "    img = renormalize.as_image(diff_image, source='pt')\n",
    "#     all_image = np.concatenate([ori_image, adv_image, diff_image], axis=1)\n",
    "    display(renormalize.as_image(ori_image, source='pt'))\n",
    "    display(renormalize.as_image(adv_image, source='pt'))\n",
    "    display(renormalize.as_image(diff_image, source='pt'))\n",
    "#     for u_idx, u in enumerate(unit_ids):\n",
    "#         print(u_idx, u)\n",
    "    display(show.blocks(\n",
    "        [[[\n",
    "           'unit {:03d} vac drop {:.3f} diff_mean {:.3f}'.format(u, unit_acc[u_idx], acts_mean_average[u]),\n",
    "           '{:s} {:3.3f} diff = {:.3f}'.format(unit_names['units'][u]['label'], unit_names['units'][u]['iou'], acts_mean[u]),\n",
    "           [iv.masked_image(image_ori, acts_ori, (0, u))],\n",
    "           [iv.heatmap(acts_ori, (0, u), mode='nearest')],\n",
    "           [iv.masked_image(adv, acts_adv, (0, u))],\n",
    "           [iv.heatmap(acts_adv, (0, u), mode='nearest')],\n",
    "          ]\n",
    "          for u_idx, u in enumerate(unit_ids)]\n",
    "        ],\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(good_indices, open('ski_resort_to_bedroom.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import foolbox\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import numpy as np\n",
    "# import cv2\n",
    "print(foolbox.__version__)\n",
    "from foolbox.criteria import TargetClass, TargetClassProbability\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "from netdissect import pbar\n",
    "\n",
    "def mkdir(path):\n",
    "    \"\"\"create a single empty directory if it didn't exist\n",
    "    Parameters:\n",
    "        path (str) -- a single directory path\n",
    "    \"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "def pickle_load(file_name):\n",
    "    data = None\n",
    "    with open(file_name, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    return data\n",
    "\n",
    "\n",
    "def pickle_save(file_name, data):\n",
    "    with open(file_name, 'wb') as f:\n",
    "        pickle.dump(data, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# test_stop = 1\n",
    "mean = np.array([0.485, 0.456, 0.406]).reshape((3, 1, 1))  #0.475, 0.441, 0.408\n",
    "std = np.array([0.229, 0.224, 0.225]).reshape((3, 1, 1))\n",
    "\n",
    "fmodel = foolbox.models.PyTorchModel(model, bounds=(0, 1), num_classes=365, preprocessing=(mean, std))\n",
    "results_dir = 'results/adv/vgg16/{:s}_images'.format(target_class)\n",
    "print(results_dir)\n",
    "mkdir(results_dir)\n",
    "print('target id {:d}, class {:s}'.format(target_class_id, target_class))\n",
    "for good_index in pbar(good_indices):\n",
    "    result_path = os.path.join(results_dir, 'image_{:d}_target_{:s}.pkl'.format(good_index, target_class))\n",
    "    if os.path.isfile(result_path):\n",
    "        continue\n",
    "    image = dataset[good_index][0]\n",
    "    image = renormalize.as_tensor(image, source=dataset, mode='pt').numpy()\n",
    "    pred = np.argmax(fmodel.forward_one(image))\n",
    "#     print('predicted class', pred, classlabels[pred])\n",
    "    attack = foolbox.attacks.CarliniWagnerL2Attack(fmodel, criterion=TargetClass(target_class_id))\n",
    "    adversarial = attack(image, pred)\n",
    "    adv_label = np.argmax(fmodel.forward_one(adversarial))\n",
    "#     print('adversarial class', adv_label, classlabels[adv_label])\n",
    "    ori_image = torch.from_numpy(image).float()\n",
    "    adv_image =  torch.from_numpy(adversarial).float()\n",
    "    pickle_save(result_path, {'image_id': good_index, 'target_id': target_class_id, 'ori': ori_image, 'adv': adv_image})\n",
    "    if good_index % 50 == 0:\n",
    "        print('process {:d}/{:d}'.format(good_index, len(good_indices)))\n",
    "        print('predicted class', pred, classlabels[pred])\n",
    "        print('adversarial class', adv_label, classlabels[adv_label])\n",
    "#     if good_index +1 >= test_stop:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "visualize activations for single layer of single image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "def pickle_load(file_name):\n",
    "    data = None\n",
    "    with open(file_name, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    return data\n",
    "\n",
    "# loading results \n",
    "results_dir = 'results/adv/vgg16/{:s}_images'.format(target_class)\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "# test_stop = 70\n",
    "# target_class = 'ski_resort'\n",
    "acts_mean_abs_all = []\n",
    "for good_index in good_indices:\n",
    "    result_path = os.path.join(results_dir, 'image_{:d}_target_{:s}.pkl'.format(good_index, target_class))\n",
    "    data = pickle_load(result_path)\n",
    "    image_id = data['image_id']\n",
    "    target_id = data['target_id']\n",
    "    ori_image = data['ori']\n",
    "    adv_image = data['adv']\n",
    "    pred_ori = model(dataset[image_id][0][None,...].cuda())\n",
    "\n",
    "    image_ori = dataset[image_id][0]\n",
    "#     out = model(image_ori[None,...].cuda())\n",
    "    acts_ori = model.retained_layer(layername).cpu()\n",
    "\n",
    "    adv = renormalize.as_tensor(adv_image, source='pt', mode='imagenet')[None,...]\n",
    "    pred_adv = model(adv.cuda())\n",
    "\n",
    "    acts_adv = model.retained_layer(layername).cpu()\n",
    "    \n",
    "    acts_mean_abs = (acts_adv - acts_ori).abs().mean(dim=(2, 3)).numpy()[0]\n",
    "    acts_mean_abs_all.append(acts_mean_abs[..., np.newaxis])\n",
    "#     if good_index >= test_stop:\n",
    "#         break\n",
    "acts_mean_abs_all = np.concatenate(acts_mean_abs_all, axis=1)\n",
    "acts_mean_average = np.mean(acts_mean_abs_all, axis=1)\n",
    "sort_ids =  np.argsort(acts_mean_average)[::-1][:10] \n",
    "print(acts_mean_average[sort_ids])\n",
    "print(sort_ids)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Diff(li1, li2): \n",
    "    return (list(set(li1) - set(li2))) \n",
    "\n",
    "print(acts_mean_average[unit_ids])\n",
    "print(unit_ids)\n",
    "print(np.mean(acts_mean_average))\n",
    "print(np.mean(acts_mean_average[unit_ids]))\n",
    "# print(np.mean(acts_mean_average[unit_ids]))\n",
    "remain_ids = Diff(range(512), unit_ids)\n",
    "print(np.mean(acts_mean_average[remain_ids]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "_ = model(dataset[image_index][0][None,...].cuda())\n",
    "image_ori = dataset[image_index][0]\n",
    "\n",
    "acts_ori = model.retained_layer(layername).cpu()\n",
    "\n",
    "display(show.blocks(\n",
    "    [[['unit {0:03d}'.format(u), '{:s} {:f}'.format(data['images'][u]['label'], data['images'][u]['iou']),\n",
    "       [iv.masked_image(image_ori, acts_ori, (0, u))],\n",
    "       [iv.heatmap(acts_ori, (0, u), mode='nearest')]]\n",
    "      for u in range(min(acts.shape[1], 32)) if data['images'][u]['iou'] > iou_threshold]\n",
    "    ],\n",
    "))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform adversarial attacks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import foolbox\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import numpy as np\n",
    "import cv2\n",
    "print(foolbox.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adversarial attack a pre-trained PyTorch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from foolbox.criteria import TargetClass, TargetClassProbability\n",
    "\n",
    "image_index=0\n",
    "\n",
    "label = dataset[image_index][1]\n",
    "image = dataset[image_index][0]\n",
    "image = renormalize.as_tensor(image, source=dataset, mode='pt').numpy()\n",
    "mean = np.array([0.485, 0.456, 0.406]).reshape((3, 1, 1))  #0.475, 0.441, 0.408\n",
    "std = np.array([0.229, 0.224, 0.225]).reshape((3, 1, 1))\n",
    "\n",
    "fmodel = foolbox.models.PyTorchModel(model, bounds=(0, 1), num_classes=365, preprocessing=(mean, std))\n",
    "print(image.max(), image.min(), image.shape, image.dtype)\n",
    "print('label', label, classlabels[label])\n",
    "\n",
    "pred = np.argmax(fmodel.forward_one(image))\n",
    "print('predicted class', pred, classlabels[pred])\n",
    "# attack = foolbox.attacks.DeepFoolAttack(fmodel)#, criterion=TargetClass(100))  #LBFGSAttack FGSM\n",
    "attack = foolbox.attacks.CarliniWagnerL2Attack(fmodel, criterion=TargetClass(100))  #LBFGSAttack FGSM\n",
    "adversarial = attack(image, pred)\n",
    "adv_label = np.argmax(fmodel.forward_one(adversarial))\n",
    "print('adversarial class', adv_label, classlabels[adv_label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize results \n",
    "from PIL import Image\n",
    "\n",
    "ori_image = torch.from_numpy(image).float()\n",
    "adv_image =  torch.from_numpy(adversarial).float()\n",
    "diff_image = adv_image - ori_image\n",
    "diff_image = diff_image/abs(diff_image).max()*0.5+0.5\n",
    "img = renormalize.as_image(diff_image, source='pt')\n",
    "display(renormalize.as_image(ori_image, source='pt'))\n",
    "display(renormalize.as_image(adv_image, source='pt'))\n",
    "display(renormalize.as_image(diff_image, source='pt'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from netdissect import imgviz\n",
    "\n",
    "adv = renormalize.as_tensor(adv_image, source='pt', mode='imagenet')[None,...]\n",
    "_ = model(adv.cuda())\n",
    "acts_adv = model.retained_layer(layername).cpu()\n",
    "iou_threshold = 0.025\n",
    "display_units = 32\n",
    "sort_method = 'diff_p'\n",
    "num_units = acts_adv.shape[1]\n",
    "if sort_method is 'id':\n",
    "    sort_ids = range(min(num_units, display_units))\n",
    "\n",
    "if sort_method is 'iou':\n",
    "    ious = [data['images'][i]['iou'] for i in range(num_units)]\n",
    "    sort_ids = np.argsort(ious)[::-1][:display_units]\n",
    "if sort_method is 'diff_n':\n",
    "#     acts_mean_abs = (acts_adv - acts_ori).abs().mean(dim=(2, 3)).numpy()[0]\n",
    "    acts_mean = (acts_adv - acts_ori).mean(dim=(2, 3)).numpy()[0]\n",
    "    sort_ids = np.argsort(acts_mean)[:display_units]\n",
    "if sort_method is 'diff_p':\n",
    "    acts_mean = (acts_adv - acts_ori).mean(dim=(2, 3)).numpy()[0]\n",
    "    sort_ids = np.argsort(acts_mean)[::-1][:display_units] \n",
    "#     print(ious[sort_ids[0]])\n",
    "\n",
    "print(sort_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "display(show.blocks(\n",
    "    [[[\n",
    "       'unit {0:03d}'.format(u),\n",
    "       '{:s} {:3.3f} diff = {:3.3f}'.format(data['images'][u]['label'],\n",
    "                          data['images'][u]['iou'], acts_mean[u]),\n",
    "       [iv.masked_image(image_ori, acts_ori, (0, u))],\n",
    "       [iv.heatmap(acts_ori, (0, u), mode='nearest')],\n",
    "       [iv.masked_image(adv, acts_adv, (0, u))],\n",
    "       [iv.heatmap(acts_adv, (0, u), mode='nearest')],\n",
    "      ]\n",
    "      for u in sort_ids if data['images'][u]['iou'] > iou_threshold]\n",
    "    ],\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "display(show.blocks(\n",
    "    [[[\n",
    "       'unit {0:03d}'.format(u),\n",
    "       '{:s} {:3.3f} diff = {:3.3f}'.format(data['images'][u]['label'],\n",
    "                          data['images'][u]['iou'], acts_mean[u]),\n",
    "       [iv.masked_image(image_ori, acts_ori, (0, u))],\n",
    "       [iv.heatmap(acts_ori, (0, u), mode='nearest')],\n",
    "       [iv.masked_image(adv, acts_adv, (0, u))],\n",
    "       [iv.heatmap(acts_adv, (0, u), mode='nearest')],\n",
    "      ]\n",
    "      for u in sort_ids if data['images'][u]['iou'] > iou_threshold]\n",
    "    ],\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect quantile statistics\n",
    "\n",
    "First, unconditional quantiles over the activations.  We will upsample them to 56x56 to match with segmentations later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbar.descnext('rq')\n",
    "def compute_samples(batch, *args):\n",
    "    image_batch = batch.cuda()\n",
    "    _ = model(image_batch)\n",
    "    acts = model.retained_layer(layername)\n",
    "    hacts = upfn(acts)\n",
    "    return hacts.permute(0, 2, 3, 1).contiguous().view(-1, acts.shape[1])\n",
    "rq = tally.tally_quantile(compute_samples, dataset,\n",
    "                          sample_size=sample_size,\n",
    "                          r=8192,\n",
    "                          num_workers=100,\n",
    "                          pin_memory=True,\n",
    "                          cachefile=resfile('rq.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_conditional_samples(batch, *args):\n",
    "    image_batch = batch.cuda()\n",
    "    _ = model(image_batch)\n",
    "    acts = model.retained_layer(layername)\n",
    "    seg = segmodel.segment_batch(renorm(image_batch), downsample=4)\n",
    "    hacts = upfn(acts)\n",
    "    return tally.conditional_samples(hacts, seg)\n",
    "\n",
    "\n",
    "condq = tally.tally_conditional_quantile(compute_conditional_samples,\n",
    "        dataset,\n",
    "        batch_size=1, num_workers=30, pin_memory=True,\n",
    "        sample_size=sample_size, cachefile=resfile('condq.npz'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Units\n",
    "\n",
    "Collect topk stats first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbar.descnext('topk')\n",
    "def compute_image_max(batch, *args):\n",
    "    image_batch = batch.cuda()\n",
    "    _ = model(image_batch)\n",
    "    acts = model.retained_layer(layername)\n",
    "    acts = acts.view(acts.shape[0], acts.shape[1], -1)\n",
    "    acts = acts.max(2)[0]\n",
    "    return acts\n",
    "topk = tally.tally_topk(compute_image_max, dataset, sample_size=sample_size,\n",
    "        batch_size=50, num_workers=30, pin_memory=True,\n",
    "        cachefile=resfile('topk.npz'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we just need to run through and visualize the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbar.descnext('unit_images')\n",
    "\n",
    "iv = imgviz.ImageVisualizer((100, 100), source=dataset, quantiles=rq,\n",
    "        level=rq.quantiles(percent_level))\n",
    "def compute_acts(image_batch):\n",
    "    image_batch = image_batch.cuda()\n",
    "    _ = model(image_batch)\n",
    "    acts_batch = model.retained_layer(layername)\n",
    "    return acts_batch\n",
    "unit_images = iv.masked_images_for_topk(\n",
    "        compute_acts, dataset, topk, k=10, num_workers=30, pin_memory=True,\n",
    "        cachefile=resfile('top10images.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for u in [10, 20, 30, 40]:\n",
    "    print('unit %d' % u)\n",
    "    display(unit_images[u])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Units\n",
    "\n",
    "Collect 99.5 quantile stats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the segmodel for segmentations.  With broden, we could use ground truth instead.\n",
    "def compute_conditional_indicator(batch, *args):\n",
    "    image_batch = batch.cuda()\n",
    "    seg = segmodel.segment_batch(renorm(image_batch), downsample=4)\n",
    "    _ = model(image_batch)\n",
    "    acts = model.retained_layer(layername)\n",
    "    hacts = upfn(acts)\n",
    "    iacts = (hacts > level_at_995).float() # indicator\n",
    "    return tally.conditional_samples(iacts, seg)\n",
    "pbar.descnext('condi995')\n",
    "condi995 = tally.tally_conditional_mean(compute_conditional_indicator,\n",
    "        dataset, sample_size=sample_size,\n",
    "        num_workers=3, pin_memory=True,\n",
    "        cachefile=resfile('condi995.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iou_995 = tally.iou_from_conditional_indicator_mean(condi995)\n",
    "unit_label_995 = [\n",
    "        (concept.item(), seglabels[concept], segcatlabels[concept], bestiou.item())\n",
    "        for (bestiou, concept) in zip(*iou_995.max(0))]\n",
    "label_list = [label for concept, label, labelcat, iou in unit_label_995 if iou > 0.04]\n",
    "display(IPython.display.SVG(experiment.graph_conceptlist(label_list)))\n",
    "len(label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from netdissect import experiment\n",
    "labelcat_list = [labelcat for concept, label, labelcat, iou in unit_label_995 if iou > 0.04]\n",
    "display(IPython.display.SVG(experiment.graph_conceptcatlist(labelcat_list)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unit_label_adaptive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show a few units with their labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for u in [10, 20, 30, 40]:\n",
    "    print('unit %d, label %s, iou %.3f' % (u, unit_label_995[u][1], unit_label_995[u][3]))\n",
    "    display(unit_images[u])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Investigate secondary labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    seg_cor = experiment.load_concept_correlation(args, segmodel, seglabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_unit_label_995 = sorted([(unit, concept, label, iou)\n",
    "    for unit, (concept, label, labelcat, iou) in enumerate(unit_label_995)\n",
    "    ], key=lambda x: -x[-1])\n",
    "\n",
    "if False:\n",
    "    count = 0\n",
    "    double_count = 0\n",
    "    multilabels = {}\n",
    "    for unit, concept, label, iou in sorted_unit_label_995:\n",
    "        if iou < 0.02:\n",
    "            continue\n",
    "        labels = [(label, iou)]\n",
    "        for c2 in iou_995[:, unit].sort(0, descending=True)[1]:\n",
    "            if c2 == concept or seg_cor[c2, concept] > 0:\n",
    "                continue\n",
    "            if iou_995[c2, unit] < 0.02:\n",
    "                break\n",
    "            labels.append((seglabels[c2], iou_995[c2, unit]))\n",
    "            break\n",
    "        multilabels[unit] = labels\n",
    "        count += 1\n",
    "        double_count += 1 if len(labels) > 1 else 0\n",
    "        print('unit %d: %s' % (unit, ', '.join(['%s: iou %.3f' % r for r in labels])))\n",
    "        if len(labels) > 1 and label == 'bed':\n",
    "            display(unit_images[unit])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('%d doubles out of %d (%.2f)' % (double_count, count, float(double_count) / count))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaptive labeling of units\n",
    "\n",
    "using conditional quantiles and IQR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    cutoff_candidates = 1 - torch.logspace(-3, math.log10(0.15), 50)\n",
    "    unit_quantile_zero = rq.normalize(torch.zeros(256))\n",
    "    unit_quantile_mask = cutoff_candidates[None,:] <= unit_quantile_zero[:,None]\n",
    "    iqr_candidates = tally.iqr_from_conditional_quantile(condq, cutoff=cutoff_candidates)\n",
    "    iou_candidates = tally.iou_from_conditional_quantile(condq, cutoff=cutoff_candidates)\n",
    "\n",
    "    # Ignore records for which unit is zeroed\n",
    "    iqr_candidates[unit_quantile_mask[:,None,:].expand(iqr_candidates.shape)] = 0\n",
    "    best_adaptive_iqr, best_iqr_choice = iqr_candidates.max(2)\n",
    "\n",
    "    # Obtain the iou at the max-iqr threshold\n",
    "    iou_at_best_iqr = iou_candidates.gather(2, best_iqr_choice[...,None])[...,0]\n",
    "\n",
    "    # Ignore records for which the max-iqr is achieved at 50-50 (typically \"painted\", or \"building\")\n",
    "    masked_iou_at_best_iqr = iou_at_best_iqr.clone()\n",
    "    # masked_iou_at_best_iqr[best_iqr_choice == len(cutoff_candidates) - 1] = 0.0\n",
    "    masked_iou_at_best_iqr\n",
    "\n",
    "    best_adaptive_iou, best_adaptive_match = masked_iou_at_best_iqr.max(1)\n",
    "    for u in range(256):\n",
    "        print(u, best_adaptive_match[u].item(),\n",
    "              seglabels[best_adaptive_match[u]],\n",
    "              best_adaptive_iou[u].item(),\n",
    "              1 - cutoff_candidates[best_iqr_choice[u, best_adaptive_match[u]]].item())\n",
    "        print(unit_label_995[u])\n",
    "        display(unit_images[u])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Get the best entity based on iqr\n",
    "cutoff_candidates = 1 - torch.logspace(-3, math.log10(0.5), 50)\n",
    "unit_quantile_zero = rq.normalize(torch.zeros(256))\n",
    "unit_quantile_mask = cutoff_candidates[None,:] <= unit_quantile_zero[:,None]\n",
    "iqr_candidates = tally.iqr_from_conditional_quantile(condq, cutoff=cutoff_candidates)\n",
    "iou_candidates = tally.iou_from_conditional_quantile(condq, cutoff=cutoff_candidates)\n",
    "best_adaptive_iqr, best_iqr_choice = iqr_candidates.max(2)\n",
    "\n",
    "# This is needed for good results:\n",
    "unmasked_iqr_candidates = iqr_candidates.clone()\n",
    "# large_concepts = (best_iqr_choice == len(cutoff_candidates) - 1)\n",
    "# iqr_candidates[large_concepts[:,:,None].expand(iqr_candidates.shape)] = 0\n",
    "# Also ignore thresholds past zero relu threshold\n",
    "iqr_candidates[unit_quantile_mask[:,None,:].expand(iqr_candidates.shape)] = 0\n",
    "best_adaptive_iqr, best_iqr_choice = iqr_candidates.max(2)\n",
    "\n",
    "# Get rid of cases where the max iqr is at the lowest threshold\n",
    "if True:\n",
    "    max_at_low_quantile_mask = (unit_quantile_mask[:,None,:]\n",
    "        .expand(best_iqr_choice.shape + (unit_quantile_mask.shape[1],))\n",
    "        .gather(2, (best_iqr_choice[:,:,None] + 1).clamp(0, 49)))[...,0]\n",
    "    iqr_candidates[max_at_low_quantile_mask[...,None].expand(iqr_candidates.shape)] = 0\n",
    "best_adaptive_iqr, best_iqr_choice = iqr_candidates.max(2)\n",
    "\n",
    "\n",
    "iqr_at_best_threshold = iqr_candidates.gather(2, best_iqr_choice[...,None])[...,0]\n",
    "iou_at_best_iqr = iou_candidates.gather(2, best_iqr_choice[...,None])[...,0]\n",
    "\n",
    "best_adaptive_iqr, best_adaptive_match = iqr_at_best_threshold.max(1)\n",
    "best_adaptive_iou = iou_at_best_iqr.gather(1, best_adaptive_match[...,None])[...,0]\n",
    "unit_label_adaptive = []\n",
    "for u in range(256):\n",
    "    unit_label_adaptive.append((\n",
    "        best_adaptive_match[u].item(),\n",
    "        seglabels[best_adaptive_match[u]],\n",
    "        segcatlabels[best_adaptive_match[u]],\n",
    "        best_adaptive_iou[u].item()\n",
    "    ))\n",
    "    if unit_label_995[u][1] == seglabels[best_adaptive_match[u]]:\n",
    "        continue\n",
    "    print('adaptive', u, best_adaptive_match[u].item(),\n",
    "          seglabels[best_adaptive_match[u]],\n",
    "          best_adaptive_iou[u].item(),\n",
    "          1 - cutoff_candidates[best_iqr_choice[u, best_adaptive_match[u]]].item())\n",
    "    print('fixed 99.5%', unit_label_995[u])\n",
    "    display(unit_images[u])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_iqr_choice.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unit_quantile_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_at_low_quantile_mask = (unit_quantile_mask[:,None,:]\n",
    "    .expand(best_iqr_choice.shape + (unit_quantile_mask.shape[1],))\n",
    "    .gather(2, (best_iqr_choice[:,:,None] + 1).clamp(0, 49)))[...,0]\n",
    "max_at_low_quantile_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_iqr_choice.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(cutoff_candidates.numpy(), iqr_candidates[65,4].numpy(), linewidth=2)\n",
    "plt.xlabel('quantile threshold for unit')\n",
    "plt.ylabel('information quality ratio')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(cutoff_candidates.numpy(), iqr_candidates[3, 2].numpy(), linewidth=2)\n",
    "plt.xlabel('quantile threshold for unit')\n",
    "plt.ylabel('information quality ratio')\n",
    "plt.title('segments vs unit at various thresholds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(cutoff_candidates.numpy(), iqr_candidates[3, 12].numpy(), linewidth=2)\n",
    "plt.xlabel('quantile threshold for unit 254')\n",
    "plt.ylabel('information quality ratio')\n",
    "plt.title('Signboard segments vs unit 254 at various thresholds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from netdissect import experiment\n",
    "labelcat_list = [labelcat for concept, label, labelcat, iou in unit_label_adaptive]#  if iou > 0.02]\n",
    "display(IPython.display.SVG(experiment.graph_conceptcatlist(labelcat_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_iqr_choice[22,1], cutoff_candidates[best_iqr_choice[22,1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unit_quantile_zero[22]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unit_quantile_zero = rq.normalize(torch.zeros(256))\n",
    "cutoff_candidates[None,:] > unit_quantile_zero[:,None]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intervention experiment\n",
    "\n",
    "Part 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Disciminant Analysis\n",
    "\n",
    "LDA of concepts -> single class.  This will give us a baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "focus_class = 'church-outdoor'\n",
    "focus_classnum = classlabels.index(focus_class)\n",
    "rcov_in_class = experiment.concept_covariance(\n",
    "    args, segmodel, seglabels, sample_size=5000,\n",
    "    filter_class=lambda x: x == focus_classnum,\n",
    "    cachefile=experiment.sharedfile('lda-%s/%s-rcov.npz' % (args.seg, focus_class)))\n",
    "rcov_out_of_class = experiment.concept_covariance(\n",
    "    args, segmodel, seglabels, sample_size=5000,\n",
    "    filter_class=(lambda x: x != focus_classnum),\n",
    "    cachefile=experiment.sharedfile('lda-%s/%s-negate-rcov.npz' % (args.seg, focus_class)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def rcov_scaled_to_unit_mean(rcov):\n",
    "    rcov = copy.copy(rcov)\n",
    "    scale = 1 / rcov._mean\n",
    "    scale[rcov._mean == 0] = 0\n",
    "    rcov._mean *= scale\n",
    "    rcov.cmom2 *= scale[:, None]\n",
    "    rcov.cmom2 *= scale[None, :]\n",
    "    return rcov\n",
    "\n",
    "def rcov_scaled_to_unit_std(rcov):\n",
    "    rcov = copy.copy(rcov)\n",
    "    std = rcov.covariance().diag().sqrt()\n",
    "    scale = std.reciprocal()\n",
    "    scale[std == 0] = 0\n",
    "    rcov._mean *= scale\n",
    "    rcov.cmom2 *= scale[:, None]\n",
    "    rcov.cmom2 *= scale[None, :]\n",
    "    return rcov\n",
    "\n",
    "# rcov_in_class = rcov_scaled_to_unit_std(rcov_in_class)\n",
    "# rcov_out_of_class = rcov_scaled_to_unit_std(rcov_out_of_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from netdissect import lda\n",
    "reload(lda)\n",
    "trans = lda.lda_transform_from_covariances([rcov_in_class, rcov_out_of_class], shrinkage=0.1,\n",
    "                                           prior=[1/365.0, 364/365.0])\n",
    "for c in trans[:,0].sort(0)[1][-20:].flip(0):\n",
    "    print(seglabels[c], c.item(), trans[c, 0].item(), rcov_in_class.mean()[c].item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Salience of concept to class\n",
    "\n",
    "Here we compute the mutual information between each visual concept and each scene class.\n",
    "(We binarize the visual concept by thresholding at some number of pixels in the image; then we compute mutual information between this binary variable and each scene category.  For each scene-concept pair we choose the threshold that maximizes mutual innformation.)\n",
    "\n",
    "Listed below are the top 3 visual concepts with highest mutual information to each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#salience = experiment.load_salience_matrix(args, segmodel, classlabels, seglabels)\n",
    "salience = experiment.load_class_concept_correlation(args, segmodel, classlabels, seglabels)\n",
    "for cls in [100, 200, 300]: # range(len(classlabels)):\n",
    "    print(classlabels[cls])\n",
    "    for mi, concept in list(zip(*salience[cls].sort(0)))[:-5-1:-1]:\n",
    "         print(mi.item(), concept.item(), seglabels[concept])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we print the salience information the other way: for each visual concept, we list the top scene categories with highest mutual information to that visual concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for concept in [5, 10, 15, 20, 25]:\n",
    "    print(seglabels[concept])\n",
    "    for mi, cls in list(zip(*salience[:,concept].sort(0)))[:-5-1:-1]:\n",
    "        print(mi.item(), cls.item(), classlabels[cls])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Per-class topk\n",
    "\n",
    "visualization over subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbar.descnext('topk in each class')\n",
    "def compute_image_max_per_class(batch, class_batch, index_batch, *args):\n",
    "    classes = class_batch.bincount().nonzero()\n",
    "    image_batch = batch.cuda()\n",
    "    _ = model(image_batch)\n",
    "    acts = model.retained_layer(layername)\n",
    "    acts = acts.view(acts.shape[0], acts.shape[1], -1)\n",
    "    acts = acts.max(2)[0].cpu()\n",
    "    for cls in classes:\n",
    "        mask = (class_batch == cls)\n",
    "        yield (cls.item(), acts[mask], index_batch[mask])\n",
    "\n",
    "topk_perclass = tally.tally_conditional_topk(compute_image_max_per_class, dataset,\n",
    "    sample_size=sample_size,\n",
    "    batch_size=50, num_workers=30, pin_memory=True,\n",
    "    cachefile=resfile('topk_perclass.npz'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization of class accuracy drop\n",
    "\n",
    "Plotting per-class accuracy drop versus salience (mutual information) ordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which classes are most salient to each concept?\n",
    "\n",
    "def align_yaxis(ax1, ax2):\n",
    "    \"\"\"Align zeros of the two axes, zooming them out by same ratio\"\"\"\n",
    "    axes = (ax1, ax2)\n",
    "    extrema = [ax.get_ylim() for ax in axes]\n",
    "    tops = [extr[1] / (extr[1] - extr[0]) for extr in extrema]\n",
    "    # Ensure that plots (intervals) are ordered bottom to top:\n",
    "    if tops[0] > tops[1]:\n",
    "        axes, extrema, tops = [list(reversed(l)) for l in (axes, extrema, tops)]\n",
    "\n",
    "    # How much would the plot overflow if we kept current zoom levels?\n",
    "    tot_span = tops[1] + 1 - tops[0]\n",
    "\n",
    "    b_new_t = extrema[0][0] + tot_span * (extrema[0][1] - extrema[0][0])\n",
    "    t_new_b = extrema[1][1] - tot_span * (extrema[1][1] - extrema[1][0])\n",
    "    axes[0].set_ylim(extrema[0][0], b_new_t)\n",
    "    axes[1].set_ylim(t_new_b, extrema[1][1])\n",
    "\n",
    "def plot_intervention_classes(concept, acc_diff, count=None, title=None):\n",
    "    most_salient_classes = salience.sort(0)[1].flip(0)\n",
    "    # concept = seglabels.index('bed')\n",
    "    dpi = 100\n",
    "    # f, (a1, a0) = plt.subplots(2, 1, gridspec_kw = {'height_ratios':[1, 2]}, dpi=dpi)\n",
    "    f, a1 = plt.subplots(1, 1, dpi=dpi, figsize=(10, 5))\n",
    "\n",
    "    x = []\n",
    "    labels = []\n",
    "    mutual_info = []\n",
    "    accuracy_diff = []\n",
    "    for cls in most_salient_classes[:,concept][:count]:\n",
    "        # pbar.print(classlabels[cls], salience[cls, concept])\n",
    "        mutual_info.append(salience[cls, concept])\n",
    "        labels.append(classlabels[cls])\n",
    "        x.append(len(x))\n",
    "        accuracy_diff.append(acc_diff[cls])\n",
    "    a1.bar(x, mutual_info)\n",
    "    a1.set_ylabel('Concept-class mutual information')\n",
    "    a1.set_xlabel('Classes ordered by correlation with concept \"%s\"' % seglabels[concept])\n",
    "    if len(x) < 60:\n",
    "        a1.set_xticks(x)\n",
    "        a1.set_xticklabels([label.replace('_', ' ') for label in labels], rotation='vertical')\n",
    "    \n",
    "    a2 = a1.twinx()\n",
    "    a2.plot(x, [-a for a in accuracy_diff], linewidth=2, color='orange')\n",
    "    a2.spines[\"right\"].set_visible(True)\n",
    "    a2.set_ylabel('Class accuracy drop')\n",
    "    if title is None:\n",
    "        title = 'Effect of zeroing detector for %s' % (seglabels[concept])\n",
    "    a2.set_title(title)\n",
    "    align_yaxis(a2, a1)\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ablation of single concept detectors.\n",
    "\n",
    "Question: when we ablate a single unit, how does it affect accuracy of each output class?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbar.descnext('baseline_acc')\n",
    "baseline_accuracy = experiment.test_perclass_accuracy(model, dataset,\n",
    "        cachefile=resfile('acc_baseline.npy'))\n",
    "pbar.print('baseline acc', baseline_accuracy.mean().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above: recall which concepts are present and seem to have one or more units specific to that concept.\n",
    "\n",
    "Below, pull out units to probe corresponding to the top 12 concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ablation_size = 5\n",
    "\n",
    "for label, group in experiment.get_top_label_unit_groups(unit_label_995,\n",
    "        size=ablation_size, num=5, min_iou=0.02):\n",
    "    concept = group[0][1]\n",
    "    pbar.descnext('test %s' % label)\n",
    "    ablation_accuracy = experiment.test_perclass_accuracy(model, dataset,\n",
    "        layername=layername,\n",
    "        ablated_units=[unit for unit, iou, concept in group],\n",
    "        cachefile=resfile('acc_ablate_%d_%s.npy' % (ablation_size, label)))\n",
    "    pbar.print('ablate %s units of %s(%d) acc %.3f %s' %\n",
    "            (len(group), label, concept, ablation_accuracy.mean().item(),\n",
    "                args.model) )\n",
    "    pbar.print(', '.join(['unit %d: iou %.3f' % (unit, iou)\n",
    "        for unit, concept, iou in group]))\n",
    "    for unit, _, _ in group:\n",
    "        print('unit %d: %s' % (unit, ', '.join(['%s: iou %.3f' % r for r in multilabels[unit]])))\n",
    "\n",
    "    unit = group[0][0]\n",
    "    display(unit_images[unit])\n",
    "    # Which classes are most damaged?\n",
    "    acc_diff = ablation_accuracy - baseline_accuracy\n",
    "    for cls in acc_diff.sort(0)[1][:5]:\n",
    "        pbar.print('%s(%d) (mi %.4f): acc %.2f -> %.2f' % (\n",
    "            classlabels[cls], cls,\n",
    "            salience[cls, concept],\n",
    "            baseline_accuracy[cls], ablation_accuracy[cls]))\n",
    "        # display(iv.masked_image_for_conditional_topk(compute_acts, dataset, topk_perclass, cls.item(), unit))\n",
    "    plot_intervention_classes(seglabels.index(label), acc_diff,\n",
    "            title='Effect of zeroing 5 units (%s %s detectors)' % (args.model, label))\n",
    "    plot_intervention_classes(seglabels.index(label), acc_diff,\n",
    "            count=50,\n",
    "            title='Effect of zeroing 5 units (%s %s detectors)' % (args.model, label))\n",
    "    pbar.print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multilabels.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ablation of single units.\n",
    "\n",
    "Question: when we ablate a single unit, how does it affect accuracy of each output class?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "top_iou_units = sorted([(unit, label, iou)\n",
    "        for unit, (concept, label, labelcat, iou) in enumerate(unit_label_995)],\n",
    "        key=lambda x: -x[-1])[:300]\n",
    "for unit, label, iou in [r for r in top_iou_units if r[0] == 48]:\n",
    "    pbar.descnext('test unit %d' % unit)\n",
    "    ablation_accuracy = experiment.test_perclass_accuracy(model, dataset,\n",
    "        layername=layername,\n",
    "        ablated_units=[unit],\n",
    "        cachefile=resfile('acc_ablate_unit_%d.npy' % (unit)))\n",
    "    pbar.print('ablate unit %d (%s iou %.3f) acc %.3f %s' %\n",
    "            (unit, label, iou, ablation_accuracy.mean().item(),\n",
    "                args.model) )\n",
    "    display(unit_images[unit])\n",
    "    # Which classes are most damaged?\n",
    "    acc_diff = ablation_accuracy - baseline_accuracy\n",
    "    for cls in acc_diff.sort(0)[1][:10]:\n",
    "        pbar.print('%s(%d) (mi %.4f): acc %.2f -> %.2f' % (\n",
    "            classlabels[cls], cls,\n",
    "            salience[cls, concept],\n",
    "            baseline_accuracy[cls], ablation_accuracy[cls]))\n",
    "        display(iv.masked_image_for_conditional_topk(compute_acts, dataset, topk_perclass, cls.item(), unit))\n",
    "    plot_intervention_classes(seglabels.index(label), acc_diff,\n",
    "            title='Effect of zeroing unit %d (%s %s detector, iou %.3f)' % (unit, args.model, label, iou))\n",
    "    plot_intervention_classes(seglabels.index(label), acc_diff,\n",
    "            count=50,\n",
    "            title='Effect of zeroing unit %d (%s %s detector, iou %.3f)' % (unit, args.model, label, iou))\n",
    "    pbar.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load all single-unit ablation perclass accuracy matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_unit_ablation_acc = torch.zeros(num_units, len(classlabels))\n",
    "\n",
    "for unit in range(num_units):\n",
    "    single_unit_ablation_acc[unit] = experiment.test_perclass_accuracy(model, dataset,\n",
    "        layername=layername,\n",
    "        ablated_units=[unit],\n",
    "        cachefile=resfile('acc_ablate_unit_%d.npy' % (unit)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ablation_delta = single_unit_ablation_acc - baseline_accuracy\n",
    "ablation_delta.max(0)[0].mean(),  ablation_delta.min(0)[0].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Focus on single discriminative class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "focus_class = 'mosque-outdoor'\n",
    "clsnum = dataset.classes.index(focus_class)\n",
    "clsnum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall on church images is 41%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_accuracy[clsnum]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discrimination = experiment.load_lda_vector(focus_class, args, segmodel, classlabels, seglabels, shrinkage=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the top concepts that are most salient to churches, just by mutual information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for concept in discrimination.sort(0)[1].flip(0)[:20]:\n",
    "    print(seglabels[concept], discrimination[concept].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for unit in ablation_delta[:,clsnum].sort(0)[1][:10]:\n",
    "    concept, label, labelcat, iou = unit_label_995[unit]\n",
    "    damage = ablation_delta[unit, clsnum]\n",
    "    print('unit %d (%s, iou %.3f) causes damage %.3f' % (unit, label, iou, damage))\n",
    "    display(unit_images[unit])\n",
    "    display(iv.masked_image_for_conditional_topk(compute_acts, dataset, topk_perclass, clsnum, unit.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unit_label_995"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize units\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_twin(triples, count=None, title=None, dpi=100, barlabel=None, linelabel=None,\n",
    "              label_ticks=True, figsize=(10, 5)):\n",
    "    ordering = [i for t, i in sorted((t, i) for i, t in enumerate(triples))[::-1]]\n",
    "    x = []\n",
    "    labels = []\n",
    "    bars = []\n",
    "    lines = []\n",
    "    for i in ordering[:count]:\n",
    "        x.append(len(x))\n",
    "        bars.append(triples[i][0])\n",
    "        lines.append(triples[i][1])\n",
    "        labels.append(triples[i][2])\n",
    "    f, a1 = plt.subplots(1, 1, dpi=dpi, figsize=figsize)\n",
    "    f.patch.set_facecolor('white')\n",
    "    a1.bar(x, bars)\n",
    "    if barlabel is not None:\n",
    "        a1.set_ylabel(barlabel)\n",
    "        a1.set_xlabel('Ordered by %s' % barlabel)\n",
    "    if label_ticks:\n",
    "        a1.set_xticks(x)\n",
    "        a1.set_xticklabels([label.replace('_', ' ') for label in labels], rotation='vertical')\n",
    "    a2 = a1.twinx()\n",
    "    a2.plot(x, lines, linewidth=2, color='orange')\n",
    "    a2.spines[\"right\"].set_visible(True)\n",
    "    if linelabel is not None:\n",
    "        a2.set_ylabel(linelabel)\n",
    "    if title:\n",
    "        a2.set_title(title)\n",
    "    align_yaxis(a2, a1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "focus_class = dataset.classes[28]\n",
    "clsnum = dataset.classes.index(focus_class)\n",
    "discrimination = experiment.load_lda_vector(focus_class, args, segmodel, classlabels, seglabels, shrinkage=0.1)\n",
    "\n",
    "clsnum\n",
    "\n",
    "def plot_intervention_units(clsnum, ablation_delta, unit_label_995, discrimination, count=None,\n",
    "                            figsize=(20, 5), label_ticks=True):\n",
    "    triples = []\n",
    "    for unit in range(256):\n",
    "        # Bardata is salience of most-salient unit concept\n",
    "        # Linedata is damage done by the unit\n",
    "        # Method 1.\n",
    "        # matching_concepts = [iou_995[:, unit].max(0)[1]]\n",
    "        matching_concepts = (iou_995[:, unit] > 0.02).nonzero()[:,0]\n",
    "        if len(matching_concepts) == 0:\n",
    "            matching_concepts = [iou_995[:, unit].max(0)[1]]\n",
    "        #    relevance, relevant_concept = (0.0, 0)\n",
    "        # else:\n",
    "        relevance, relevant_concept = max([(discrimination[c, 0], c) for c in matching_concepts])\n",
    "        relevance = relevance.item()\n",
    "        bar = relevance\n",
    "        label = '%s (%d)' % (seglabels[relevant_concept], unit)\n",
    "        line = -ablation_delta[unit, clsnum].item()\n",
    "        triples.append((bar, line, label))\n",
    "    plot_twin(triples, count=count, figsize=figsize, label_ticks=label_ticks,\n",
    "              title=\"Can we predict how much a unit will damage %s classification accuracy?\" % focus_class,\n",
    "              barlabel=\"salience of unit concept to %s (bars)\" % focus_class,\n",
    "              linelabel=\"damage to accuracy of %s (line)\" % focus_class)\n",
    "\n",
    "plot_intervention_units(clsnum, ablation_delta, unit_label_995, discrimination, count=256, figsize=(50, 5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unit = 11\n",
    "matching_concepts = (iou_995[:, unit] > 0.02).nonzero()[:,0]\n",
    "relevance, relevant_concept = max([(discrimination[c, 0], c) for c in matching_concepts])\n",
    "seglabels[relevant_concept], relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for clsnum in range(0, len(classlabels), 10):\n",
    "    focus_class = dataset.classes[clsnum]\n",
    "    print(focus_class)\n",
    "    # clsnum = dataset.classes.index(focus_class)\n",
    "    discrimination = experiment.load_lda_vector(focus_class, args, segmodel, classlabels, seglabels, shrinkage=0.1)\n",
    "    plot_intervention_units(clsnum, ablation_delta, unit_label_995, discrimination, count=256, label_ticks=False)\n",
    "    plot_intervention_units(clsnum, ablation_delta, unit_label_995, discrimination, count=80, label_ticks=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a table of concepts that are most disciminative\n",
    "discriminate_matrix = torch.zeros(len(classlabels), len(seglabels))\n",
    "for clsnum in range(len(classlabels)):\n",
    "    focus_class = dataset.classes[clsnum]\n",
    "    d = experiment.load_lda_vector(focus_class, args, segmodel, classlabels, seglabels, shrinkage=0.1)\n",
    "    discriminate_matrix[clsnum] = d[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unit_damage_matrix = torch.zeros(len(classlabels), 256)\n",
    "for unit in range(256):\n",
    "    ablation_accuracy = experiment.test_perclass_accuracy(model, dataset,\n",
    "        layername=layername,\n",
    "        ablated_units=[unit],\n",
    "        cachefile=resfile('acc_ablate_unit_%d.npy' % (unit)))\n",
    "    acc_diff = ablation_accuracy - baseline_accuracy\n",
    "    unit_damage_matrix[:,unit] = acc_diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding:\n",
    "\n",
    "On average, zeroing a unit that detects the most discriminative concept for a class (for the 153 classes for which there is a unit for the most discriminative concept) damages accuracy of classification of class by an average of 4.1%, whereas zeroing other units damages accuracy of that class only by an average of 0.05%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Idea: for each unit, salience maybe shold be given by the most salient high-iou concept detected by the unit.\n",
    "relevant_units = []\n",
    "all_other_units = []\n",
    "counted_classes = 0\n",
    "\n",
    "for clsnum in range(len(classlabels)):\n",
    "    segnum = discriminate_matrix[clsnum].max(0)[1]\n",
    "    # print('Most relevant to %s is %s' % (classlabels[clsnum], seglabels[segnum]))\n",
    "    units = [unit\n",
    "             for unit, (s, _, _, iou) in enumerate(unit_label_995)\n",
    "             if s == segnum\n",
    "             if iou > 0.03\n",
    "            ]\n",
    "    if not len(units):\n",
    "        continue\n",
    "    counted_classes += 1\n",
    "    other_units = [u for u in range(256) if u not in units]\n",
    "    # print(', '.join(str(r) for r in units))\n",
    "    relevant_units.extend(unit_damage_matrix[clsnum, units].numpy().tolist())\n",
    "    all_other_units.extend(unit_damage_matrix[clsnum, other_units].numpy().tolist())\n",
    "\n",
    "print('Counted %d classes' % counted_classes)\n",
    "print('Of the %d most relevant units, average damage is %.3g' %\n",
    "      (len(relevant_units), torch.tensor(relevant_units).mean().item()))\n",
    "print('Of the %d most other units, average damage is %.3g' %\n",
    "      (len(all_other_units), torch.tensor(all_other_units).mean().item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second experiment: compare ablation of the most-relevant concept detectors, where the most-relevant concept is counted among only those concepts that exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_units = []\n",
    "all_other_units = []\n",
    "\n",
    "iou_floor = 0.03\n",
    "\n",
    "for clsnum in range(len(classlabels)):\n",
    "    # segnum = discriminate_matrix[clsnum].max(0)[1]\n",
    "    # even if there is not a unit for the most discriminate feature, find unit\n",
    "    # for the most discriminate feature for which there is a unit.\n",
    "    units = sorted([(-discriminate_matrix[clsnum, s], s, unit)\n",
    "             for unit, (s, _, _, iou) in enumerate(unit_label_995)\n",
    "             if iou > iou_floor\n",
    "             ])\n",
    "    segnum = units[0][1]\n",
    "    units = [unit\n",
    "             for unit, (s, _, _, iou) in enumerate(unit_label_995)\n",
    "             if s == segnum\n",
    "             and iou > iou_floor\n",
    "            ]\n",
    "    other_units = [u for u in range(256) if u not in units]\n",
    "    # print('Most relevant to %s is %s (%d units)' % (classlabels[clsnum], seglabels[segnum], len(units)))\n",
    "    # print(', '.join(str(r) for r in units))\n",
    "    relevant_units.extend(unit_damage_matrix[clsnum, units].numpy().tolist())\n",
    "    all_other_units.extend(unit_damage_matrix[clsnum, other_units].numpy().tolist())\n",
    "print('Of the %d most relevant units, average damage is %.3g' %\n",
    "      (len(relevant_units), torch.tensor(relevant_units).mean().item()))\n",
    "print('Of the %d most other units, average damage is %.3g' %\n",
    "      (len(all_other_units), torch.tensor(all_other_units).mean().item()))\n",
    "\n",
    "print('Ratio %.3f' % (\n",
    "    torch.tensor(relevant_units).mean().item() / torch.tensor(all_other_units).mean().item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Third idea: for each class, order units according to the salience of the detected concept, and; and then average the impacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "all_sorted_damage = torch.zeros(len(classlabels), 256)\n",
    "all_unit_concepts = set(u[1] for u in unit_label_995)\n",
    "for clsnum in range(len(classlabels)):\n",
    "    unit_sorter = sorted([(-discriminate_matrix[clsnum, s], s, unit)\n",
    "             for unit, (s, _, _, iou) in enumerate(unit_label_995)\n",
    "             ])\n",
    "    unit_order = [u[-1] for u in unit_sorter]\n",
    "    unit_concept = [u[1] for u in unit_sorter]\n",
    "    sorted_damage = unit_damage_matrix[clsnum, unit_order]\n",
    "    # since units with the same concept could be listed in any order, average their contributionns\n",
    "    for s in all_unit_concepts:\n",
    "        sorted_damage[unit_concept == s] = sorted_damage[unit_concept == s].mean()\n",
    "    all_sorted_damage[clsnum] = sorted_damage\n",
    "\n",
    "f, a1 = plt.subplots(1, 1, dpi=200, figsize=(10, 5))\n",
    "a1.bar(range(51), (-all_sorted_damage.mean(0).numpy()[:50] * 100).tolist() +\n",
    "       [-all_sorted_damage.mean(0).numpy()[50:].mean() * 100])\n",
    "\n",
    "a1.set_title('Effect of removing an object detector unit on classification accuracy of a scene class')\n",
    "a1.set_ylabel('Damage to classification accuracy of scene class\\nwhen a single unit is zeroed, percent')\n",
    "a1.set_xlabel('Units ordered by (LDA-determined) salience of detected object to the affected scene class')\n",
    "a1.set_xticks([0, 9, 19, 29, 39, 50])\n",
    "a1.set_xticklabels(['1', '10', '20', '30', '40',  '>50'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adaptive case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sorted_damage = torch.zeros(len(classlabels), 256)\n",
    "all_unit_concepts = set(u[1] for u in unit_label_adaptive)\n",
    "for clsnum in range(len(classlabels)):\n",
    "    unit_sorter = sorted([(-discriminate_matrix[clsnum, s], s, unit)\n",
    "             for unit, (s, _, _, iou) in enumerate(unit_label_adaptive)\n",
    "             ])\n",
    "    unit_order = [u[-1] for u in unit_sorter]\n",
    "    unit_concept = [u[1] for u in unit_sorter]\n",
    "    sorted_damage = unit_damage_matrix[clsnum, unit_order]\n",
    "    # since units with the same concept could be listed in any order, average their contributionns\n",
    "    for s in all_unit_concepts:\n",
    "        sorted_damage[unit_concept == s] = sorted_damage[unit_concept == s].mean()\n",
    "    all_sorted_damage[clsnum] = sorted_damage\n",
    "\n",
    "f, a1 = plt.subplots(1, 1, dpi=200, figsize=(10, 5))\n",
    "a1.bar(range(51), (-all_sorted_damage.mean(0).numpy()[:50] * 100).tolist() +\n",
    "       [-all_sorted_damage.mean(0).numpy()[50:].mean() * 100])\n",
    "\n",
    "a1.set_title('Effect of removing an object detector unit on classification accuracy of a scene class')\n",
    "a1.set_ylabel('Damage to classification accuracy of scene class\\nwhen a single unit is zeroed, percent')\n",
    "a1.set_xlabel('Units ordered by (LDA-determined) salience of detected object to the affected scene class')\n",
    "a1.set_xticks([0, 9, 19, 29, 39, 50])\n",
    "a1.set_xticklabels(['1', '10', '20', '30', '40',  '>50'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fourth idea: scatterplot.  Salience rank of a unit on the x axis, and classification accuracy damage on the y axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sorted_damage = torch.zeros(len(classlabels), 256)\n",
    "all_unit_concepts = set(u[1] for u in unit_label_995)\n",
    "yvals = []\n",
    "xvals = []\n",
    "for clsnum in range(len(classlabels)):\n",
    "    unit_sorter = sorted([(-discriminate_matrix[clsnum, s], s, unit)\n",
    "             for unit, (s, _, _, iou) in enumerate(unit_label_995)\n",
    "             ])\n",
    "    unit_order = [u[-1] for u in unit_sorter]\n",
    "    unit_concept = [u[1] for u in unit_sorter]\n",
    "    sorted_damage = unit_damage_matrix[clsnum, unit_order]\n",
    "    rank_order = torch.arange(len(sorted_damage), dtype=torch.float)\n",
    "    # since units with the same concept could be listed in any order, average their contributionns\n",
    "    for s in all_unit_concepts:\n",
    "        rank_order[unit_concept == s] = rank_order[unit_concept == s].mean()\n",
    "    xvals.extend(rank_order.numpy().tolist())\n",
    "    yvals.extend(sorted_damage.numpy().tolist())\n",
    "    all_sorted_damage[clsnum] = sorted_damage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "f, a1 = plt.subplots(1, 1, dpi=200, figsize=(30, 5))\n",
    "a1.scatter([x + random.random() for x in xvals],\n",
    "           [y + random.random() * 0.01 for y in yvals], s=0.5, alpha=0.2)\n",
    "a1.set_xlabel('Units ordered by salience of detected object to the affected scene class using LDA')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(u, iou) for u, (s, label, labelcat, iou) in enumerate(unit_label_995) if label == 'bed']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fifth idea, similar to the \"Effect\" graph, but here the x axis is purely determined by LDA and has nothing to do with the network being teested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_sorted_damage = torch.zeros(len(classlabels), len(seglabels))\n",
    "count_sorted_damage = torch.zeros(len(classlabels), len(seglabels))\n",
    "\n",
    "for clsnum in range(len(classlabels)):\n",
    "    dscore, drank = (-discriminate_matrix[clsnum]).sort(0)\n",
    "    zerorank = (dscore == 0).nonzero()[:, 0].numpy().tolist()\n",
    "    rankmap = {s.item(): zerorank if r in zerorank else [r] for r, s in enumerate(drank)}\n",
    "    for u, (s, label, labelcat, iou) in enumerate(unit_label_995):\n",
    "        r = rankmap[s]\n",
    "        damage = unit_damage_matrix[clsnum, u]\n",
    "        total_sorted_damage[clsnum, r] += (damage / len(r))\n",
    "        count_sorted_damage[clsnum, r] += (1.0 / len(r))\n",
    "avg_sorted_damage = (total_sorted_damage.sum(0) / count_sorted_damage.sum(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, a1 = plt.subplots(1, 1, dpi=200, figsize=(10, 5))\n",
    "a1.bar(range(len(avg_sorted_damage)), -avg_sorted_damage.numpy() * 100)\n",
    "f.show()\n",
    "\n",
    "f, a1 = plt.subplots(1, 1, dpi=200, figsize=(10, 5))\n",
    "a1.bar(range(len(count_sorted_damage.sum(0))), count_sorted_damage.sum(0).numpy())\n",
    "f.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, a1 = plt.subplots(1, 1, dpi=200, figsize=(10, 5))\n",
    "a1.bar(range(51), (-avg_sorted_damage.numpy()[:50] * 100).tolist() +\n",
    "       [-avg_sorted_damage.numpy()[50:].mean() * 100])\n",
    "a1.set_ylabel('Damage to classification accuracy of scene class\\nwhen a single unit is zeroed, percent')\n",
    "a1.set_xlabel('Which object detector is zeroed, identified by dissection, ordered by LDA salience of the object to the scene')\n",
    "a1.set_xticks([0, 9, 19, 29, 39, 50])\n",
    "a1.set_xticklabels(['1', '10', '20', '30', '40',  '>50'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sixth idea, put average rank by LDA on the y axis, and put rank by intervention impact on the x axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_rank_for_unit = torch.zeros(len(classlabels), 256)\n",
    "lda_count_for_unit = torch.zeros(len(classlabels), 256)\n",
    "\n",
    "for clsnum in range(len(classlabels)):\n",
    "    damage, damrank = (unit_damage_matrix[clsnum]).sort(0)\n",
    "    rankmap = {damrank[r].item(): (damage == d).nonzero()[:, 0].numpy().tolist() for r, d in enumerate(damage)}\n",
    "    \n",
    "    dscore, drank = (-discriminate_matrix[clsnum]).sort(0)\n",
    "    # TODO: handle zero rank\n",
    "    # zerorank = (dscore == 0).nonzero()[:, 0].numpy().mean()\n",
    "    srankmap = {s.item(): r for r, s in enumerate(drank)}\n",
    "\n",
    "    for u, (s, label, labelcat, iou) in enumerate(unit_label_995):\n",
    "        ur = rankmap[u]\n",
    "        sr = srankmap[s]\n",
    "        \n",
    "        damage = unit_damage_matrix[clsnum, u]\n",
    "        lda_rank_for_unit[clsnum, ur] += (sr / len(ur))\n",
    "        lda_count_for_unit[clsnum, ur] += (1.0 / len(ur))\n",
    "        \n",
    "avg_lda_rank_for_unit = (lda_rank_for_unit.sum(0) / lda_count_for_unit.sum(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_lda_rank_for_unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, a1 = plt.subplots(1, 1, dpi=200, figsize=(10, 5))\n",
    "a1.plot(range(51), (avg_lda_rank_for_unit.numpy()[:50]).tolist() +\n",
    "       [avg_lda_rank_for_unit.numpy()[50:].mean()])\n",
    "a1.set_ylabel('Average rank of object detected by unit,\\nordered by salience to class')\n",
    "a1.set_xlabel('Which unit zeroed, ordered by damage caused to the scene class')\n",
    "a1.set_xticks([0, 9, 19, 29, 39, 50])\n",
    "a1.set_xticklabels(['1', '10', '20', '30', '40',  '>50'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seventh idea, LDA coefficient on y axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_weight_for_unit = torch.zeros(len(classlabels), 256)\n",
    "lda_count_for_unit = torch.zeros(len(classlabels), 256)\n",
    "\n",
    "for clsnum in range(len(classlabels)):\n",
    "    damage, damrank = (unit_damage_matrix[clsnum]).sort(0)\n",
    "    rankmap = {damrank[r].item(): (damage == d).nonzero()[:, 0].numpy().tolist() for r, d in enumerate(damage)}\n",
    "    \n",
    "    ldavec = discriminate_matrix[clsnum]\n",
    "    ldavec /= ldavec.max()\n",
    "    dscore, drank = (-ldavec).sort(0)\n",
    "    dscore = -dscore\n",
    "    # TODO: handle zero rank\n",
    "    # zerorank = (dscore == 0).nonzero()[:, 0].numpy().mean()\n",
    "    srankmap = {s.item(): sc.item() for s, sc in zip(drank, dscore)}\n",
    "\n",
    "    for u, (s, label, labelcat, iou) in enumerate(unit_label_995):\n",
    "        ur = rankmap[u]\n",
    "        sr = srankmap[s]\n",
    "        \n",
    "        damage = unit_damage_matrix[clsnum, u]\n",
    "        lda_weight_for_unit[clsnum, ur] += (sr / len(ur))\n",
    "        lda_count_for_unit[clsnum, ur] += (1.0 / len(ur))\n",
    "        \n",
    "avg_lda_weight_for_unit = (lda_weight_for_unit.sum(0) / lda_count_for_unit.sum(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_weight_for_unit.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, a1 = plt.subplots(1, 1, dpi=200, figsize=(10, 5))\n",
    "a1.plot(range(51), (avg_lda_weight_for_unit.numpy()[:50]).tolist() +\n",
    "       [avg_lda_weight_for_unit.numpy()[50:].mean()], linewidth=2,\n",
    "        label=\"LDA salience of objects detected by units with largest causal effect on class accuracy.\")\n",
    "a1.plot(range(51), 51 * [lda_weight_for_unit.mean().item()], linewidth=2, alpha=0.7,\n",
    "       label=\"Mean LDA salience for random units %.2g.  (Object with maximum salience is 1.0.)\" %\n",
    "        lda_weight_for_unit.mean().item())\n",
    "a1.set_ylabel('Average LDA salience of object detected by zeroed unit')\n",
    "a1.set_xlabel('Which unit zeroed, ordered by damage caused to the scene class')\n",
    "a1.set_xticks([0, 9, 19, 29, 39, 50])\n",
    "a1.set_xticklabels(['1', '10', '20', '30', '40',  '>50'])\n",
    "f.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat the experiment for adaptive case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lda_weight_for_unit = torch.zeros(len(classlabels), 256)\n",
    "lda_count_for_unit = torch.zeros(len(classlabels), 256)\n",
    "\n",
    "for clsnum in range(len(classlabels)):\n",
    "    damage, damrank = (unit_damage_matrix[clsnum]).sort(0)\n",
    "    rankmap = {damrank[r].item(): (damage == d).nonzero()[:, 0].numpy().tolist() for r, d in enumerate(damage)}\n",
    "    \n",
    "    ldavec = discriminate_matrix[clsnum]\n",
    "    ldavec /= ldavec.max()\n",
    "    dscore, drank = (-ldavec).sort(0)\n",
    "    dscore = -dscore\n",
    "    # TODO: handle zero rank\n",
    "    # zerorank = (dscore == 0).nonzero()[:, 0].numpy().mean()\n",
    "    srankmap = {s.item(): sc.item() for s, sc in zip(drank, dscore)}\n",
    "\n",
    "    for u, (s, label, labelcat, iou) in enumerate(unit_label_adaptive):\n",
    "        ur = rankmap[u]\n",
    "        sr = srankmap[s]\n",
    "        \n",
    "        damage = unit_damage_matrix[clsnum, u]\n",
    "        lda_weight_for_unit[clsnum, ur] += (sr / len(ur))\n",
    "        lda_count_for_unit[clsnum, ur] += (1.0 / len(ur))\n",
    "        \n",
    "avg_lda_weight_for_unit_adaptive = (lda_weight_for_unit.sum(0) / lda_count_for_unit.sum(0))\n",
    "f, a1 = plt.subplots(1, 1, dpi=200, figsize=(10, 5))\n",
    "a1.plot(range(51), (avg_lda_weight_for_unit_adaptive.numpy()[:50]).tolist() +\n",
    "       [avg_lda_weight_for_unit_adaptive.numpy()[50:].mean()], linewidth=2,\n",
    "        label=\"LDA salience of objects detected by units with largest causal effect on class accuracy.\")\n",
    "a1.plot(range(51), 51 * [lda_weight_for_unit.mean().item()], linewidth=2, alpha=0.7,\n",
    "       label=\"Mean LDA salience for objects detected by random units\")\n",
    "a1.set_ylabel('Average LDA salience of object detected by zeroed unit')\n",
    "a1.set_xlabel('Which unit zeroed, ordered by damage caused to the scene class')\n",
    "a1.set_xticks([0, 9, 19, 29, 39, 50])\n",
    "a1.set_xticklabels(['1', '10', '20', '30', '40',  '>50'])\n",
    "f.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}